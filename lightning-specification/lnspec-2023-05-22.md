---
title: Lightning Specification Meeting
transcript_by: carlaKC via TBTBTC v1.0.0
tags: ['lightning']
date: 2023-05-22
---

Speaker 0: 00:00:09

Should we get started? Are we missing LND and core lightning people?

Speaker 1: 00:00:18

Yep, looks like it.

Speaker 0: 00:00:20

So we should make all the decisions quick. Perfect.

Speaker 1: 00:00:25

Let's merge a lot of

Speaker 0: 00:00:41

stuff.

Speaker 2: 00:01:06

I'm

Speaker 0: 00:01:06

in and out. I'm sort of on vacation today, but I'll be listening. So don't wait for me to.

Speaker 1: 00:01:15

All right.

Speaker 0: 00:01:45

Silence. Thank you. All

Speaker 1: 00:02:50

right. Looks like we have a lot of people. Let's start. So the first PR we have on the list is a clarification on Bolt 8 by Rusty. It looks like, if I understand correctly, someone tried to reimplement bolt 8 and got it wrong and it's only clarifications. Is that correct?

Speaker 3: 00:03:09

Yes, yeah. So ln message, a JavaScript library that speaks to speaks to nodes, had everything right except for the fact they shared a chaining key, which is correct to start with. They start at the same value, but there's actually a separate one. And the spec was not really clear. And so, of course, it worked for the first 500 or so messages, and then it failed. So I thought, given that we have an implementation that actually failed, I looked back at the spec and went, OK, I think I see how they made that mistake. So it just clarifies there are two separate keys, one for see, one for send.

Speaker 1: 00:03:51

And did they have a look at your PR and did they say that it was indeed clarifying the issues they had?

Speaker 0: 00:04:00

No,

Speaker 3: 00:04:08

I will ask them to look. I actually found a bug in the message with Shahana. So they didn't have to go back to first principles.

Speaker 0: 00:04:18

Okay.

Speaker 1: 00:04:21

All right. So, I think it's quite simple. Maybe someone from LND or LDK can have a look as well and hack it if it looks like it matches what they've implemented. That would be great. But probably not to do right now but maybe when people have a bit of time. I realized that the next topic is the CLTV deltas for rad blinding we discussed it last time but I really didn't have time to do what I said I would do before this meeting so we still have to write I think both Matt and I have to write some clarification on what we mean so I don't think we have anything

Speaker 0: 00:05:01

yeah I've been traveling a bunch um so I have not had a chance to do that.

Speaker 1: 00:05:07

Yeah, me neither. So I'll get to it and I'll do it this week so that next time we can actually really discuss that. So let's keep it for today.

Speaker 2: 00:05:19

Right,

Speaker 1: 00:05:19

so next up is Onion Messages and I think we're ready to merge Onion Messages. Is there anything outstanding?

Speaker 3: 00:05:33

No, I agree.

Speaker 1: 00:05:36

Yeah, I think we can just squash the two fix up commits. And then maybe if we have an act from one of our team, LND or LDK, and to

Speaker 0: 00:05:45

make

Speaker 1: 00:05:46

sure that we didn't miss anything, then I think we should be ready to go. Which means that offers will finally be based on master, which means that offer is next. Yes. Finally. Because on the LND and LDK side, is there anything, I guess the implementations are complete on both implementations, so it's just a matter of verifying the test vectors. Can

Speaker 0: 00:06:18

someone

Speaker 1: 00:06:18

from one of the teams chime in? Oh, but there is, I think there's a test vector.

Speaker 4: 00:06:24

Oh, sorry, I mean we need to implement them on the LDK side.

Speaker 0: 00:06:29

Okay,

Speaker 1: 00:06:29

and Is it something you plan on doing soon or would you have time to do it before the next meeting?

Speaker 4: 00:06:37

Let me comment on the PR in the next day about that. But yes, love to. But we've done like interop testing. So that's good. Pretty confident. Perfect.

Speaker 0: 00:06:48

All right.

Speaker 1: 00:06:52

So anything else on the Messages or should we move on to

Speaker 0: 00:06:58

Authors?

Speaker 1: 00:07:02

Perfect. And anything new on the other side?

Speaker 3: 00:07:07

No, just minor clean-out, clean-out, rebasing, things like that. We had, there was a requirement to update the test vectors, but I'm trying to figure out if I've actually pushed that or

Speaker 0: 00:07:19

not. Hey, y'all can you hear me? Yeah. Cool. I

Speaker 5: 00:07:27

could check the current vectors versus what we have to see if anything has changed. If it wants, we could build.

Speaker 0: 00:07:36

I

Speaker 5: 00:07:36

also noticed today the invoice error message has a couple of fields that aren't really well-defined as far as reading goes. It's basically the, I think, erroneous or suggested value in erroneous field, maybe it's something like that. And there's some rationale about, you know, for these offerless invoice requests, maybe we need this, But it seems like we should maybe just drop those fields entirely until we have something concrete to work with, and maybe just have an error message in that.

Speaker 3: 00:08:10

Yeah, I mean, they're completely informational fields at this point. I mean, we did make this mistake with our general errors where we don't you get an error back, but it's just a text message. We don't doesn't say exactly what it's complaining about. So this is basically the same thing left forward. Yeah, I mean, any of them, they're optional for an implementation, and you can you can ignore them anyway, but they are, they're, they're kind of useful, like say, this is the field that I didn't like. And optionally, here is a better, here's what I expected in this field, or here is like the minimum I expected or something like that. But they are by definition a little bit vague. And certainly, I mean, we can, I'll just check that there are odd fields, but it's easy. So is

Speaker 2: 00:08:56

that the idea is to add structured errors to like onion message offers error replies, basically?

Speaker 0: 00:09:02

Yeah,

Speaker 3: 00:09:02

if they ask for an invoice or something and there's an error with it, you can actually, as well as having a string saying, hey, here's what I didn't like, you can actually specify, because it's a TLV, right? You have a, unless it's grossly malformed, like you have a field number. So you can give some programmatic feedback, say this is the field I don't like, which is something we want to do in like normal error messages. But yeah, it's

Speaker 0: 00:09:28

how is the sender expected to handle that? Right? If you say like, hey, your TLV field 42 is is bad. How is this? And is the sender expected to have some kind of programmatic response to say like, oh, I can decrease 42?

Speaker 2: 00:09:42

Not

Speaker 3: 00:09:43

necessarily. I mean, it's more it's more debugging aid. There is the only case where it is useful is when you have basically a currency conversion issue. So if your offer is in a currency that is, you know, not Sats, then you end up, there are cases where you will send an invoice and it will go, that's not enough, right? Or you will send an offer to pay kind of thing. So there was a corner case there where it's like, okay, well, it's nice to have a programmatic way to say, here's the number I expected for this. But it's really a corner case that isn't, you know, isn't particularly important at the moment. It's more debugging aid to say, hey, this is, you know, this is what went wrong in some systemic way. But it does open the door to the idea that we could do something smarter in future. Probably working around bugs and implementations, really. Oh, I don't know what this error message means and stuff like that. But it's optional.

Speaker 2: 00:10:40

I like it. Yeah, this means LND won't send internal error. So we can give you some more detail.

Speaker 3: 00:10:49

Yeah, so ideally you can complain about what's wrong. It seemed elementary, but yeah, almost by definition it's ill-defined because errors are like almost always shouldn't happen. And

Speaker 5: 00:11:04

So I guess the idea is that we're gonna make this very general still like it is now and not have very specific types of errors in the future. That's the

Speaker 3: 00:11:12

idea. That's right. Yeah, I expect that we'll tighten this as we go, right? If there's some really useful, feel free, we'll start tightening the requirements. But for now it's like just, yeah, optionally fill this in, it gives a clue.

Speaker 1: 00:11:26

Also I'm curious what you guys are currently doing when you get an offer and you are, are you directly connecting to the introduction point of the blended path or to the offer node or are you doing pathfinding to reach that first node?

Speaker 3: 00:11:42

We pathfind, but we currently only don't support MPP when we have the, in that case, which is really dumb. Like it's a minimal possible implementation. We're working on this new pay plugin that does all the Rene stuff, and that will solve that problem a lot more nicely.

Speaker 1: 00:12:00

Okay, because we started doing that and we wanted to just reuse our pathfinding code and we realized that there are a few edge cases because if you do a simple Dijkstra, the issue is that some channels are potentially disabled in one direction if the channel is empty whereas there's still a connection between those two. So you still want to have that edge in your graph and be able to use it for your messages because you

Speaker 0: 00:12:21

don't think

Speaker 1: 00:12:22

that the channel is actually empty. So we were not sure how much we should change the extra pathfinding to fit on your messages better or if you should implement something quite different. So that's why I was curious to know what other people did.

Speaker 0: 00:12:39

Yeah, we haven't landed it yet to do pathfinding, but we're just doing an entirely new Dijkstra implementation because this one can be so much more naive right like Our existing router has so much logic around like oh we hit HVLC min or max or blah blah blah So yeah, we're just doing a like here's a straight line Dykstra copied out of a textbook

Speaker 1: 00:13:01

Okay, yeah,

Speaker 3: 00:13:02

I was

Speaker 5: 00:13:02

just pull that

Speaker 3: 00:13:03

basically anyway, so so we just plug in a really dumb evaluation function and it just works, right?

Speaker 2: 00:13:09

Yeah, I was going to say, I think the edit disabled thing could be a hint at least, because that could be a hint for the PR just being precisely offline, which maybe you want to avoid because then that like, you know, is another round trip for you to like retry or something like that. But yeah, there probably isn't as much overlap, but some things can be helpful, I think, on the routing policy layer at least. Yeah,

Speaker 1: 00:13:28

and I think that as well, for example, people who have been around for a while channels that have been online that have existed for a long time indicate that the peers are mostly Online and maybe you want to take that edge on your messages as well So those are the things that make it potentially useful to reuse some of the Pathfinding code and heuristics. Yeah,

Speaker 3: 00:13:48

and it's tempting to say, well, this is- It already has

Speaker 2: 00:13:50

like the history, so you can use that too. Okay, they forwarded five minutes ago at least.

Speaker 3: 00:13:54

Yeah, yeah, the other thing is, yeah, using the fact that you're probably gonna make a payment after this, right? If you're requesting an invoice, right? So, you know, it's a free probe, you might as well use something similar to what you're going to use for the payment. I mean, you don't necessarily quite what the payment size is, necessarily, but at least you can come up with some, you know, vague idea. And then you can get an idea of what,

Speaker 0: 00:14:15

what you

Speaker 3: 00:14:15

know, If any of it's down, you find out early. But we're not that smart. We just do it. I need to try that. If that fails, if we can't find a route, we then redirect connect. What we could do is connect to a neighbor that forwards any messages, but we don't do that yet.

Speaker 1: 00:14:33

All right. Okay. Anything else on the files?

Speaker 5: 00:14:41

The test vector, the first one in format string test, Jason, I believe is not valid. Looks like it needs to be updated.

Speaker 3: 00:14:50

Okay, yes, I had an update. I'm not sure I pushed it. I will put that on my to-do today. Okay,

Speaker 0: 00:14:56

thanks. All

Speaker 1: 00:14:59

right, so next step is still the dust exposure threshold PR. So it's on everyone's to-do list. So maybe someday we'll have time to review

Speaker 2: 00:15:08

it. Yeah, this is on the to-do list, but I was thinking about some stuff earlier this week, basically. Because we had some reports around things like force closes. And I think there were some, but I think they also hit, they also staying harder because fees will be higher. So something you didn't know before now maybe costs you some money. But I was thinking, I remember in the past, I think we were discussing basically adding another dust limit. Or sorry, not another dust limit, but adding another value to track the dust slots. So a dust is into 483. The only thing I was looking at doing, because remember in the past, we tried to do this thing where we basically would only go to chain if it made sense as far as you're getting enough fees out of it, basically factoring the chain cost or economic chain close. Then we ended up abandoning that because we were like, okay, well, we would hold onto the incoming HLC, which doesn't make sense, right? But then I wanted to do another version that basically said, okay, like, the idea is that like, let's say I have like a hundred social, you know, HLC, right? And it's about to expire, right? And I would basically potentially, you know, go to chain for that. Instead, what I, a prototype, basically we just cancel it back in the incoming and hold onto the outgoing, right? So effectively, we're potentially, we're potentially holding onto the bag for that cost, but we've at least freed up the HLC on the incoming link basically. And in the future, maybe the outgoing one comes online, actually cancels it back, but that's fine because we're saying, okay, well, losing 100 HLC is better than losing 5k SATs on chain. And I remember there's some questions around, is this a spectrum pad or whatever else, but basically the idea is that you don't necessarily go to on-chain for dust, but you're potentially committing to losing that value in the future. And obviously the trade-off here is that, this does take up one of your 483 slots, but if there was another slot for dust, it wouldn't necessarily do that. But this is just something we're looking at because it doesn't really make sense to go to chain for that thing. Particularly because as soon as you go to chain, you can cancel it back, but still, is it worth the opportunity cost basically of paying 10k sats to get a one sat fee? I know this is something that I think T-Best want to talk about in the spec meeting around that, but this is basically, it seems like a strategy that you can do today that you need basically some like, you know, parameters on like your risk model basically around like how many you want to keep alive, what point do you go to chamber or else, but this can at least save some pain in the short term.

Speaker 0: 00:17:18

That's something we're, well, we haven't had many discussions about it, but we intend to have discussions about too. I mean, I think everyone at this point has that kind of like max dust exposure tracking, right? So you only accept up to some amount of dust. The other thing you can do is just say like, all right, I've hit my max dust exposure, I should have force closed, but this doesn't make sense, and so I'm just gonna sit here and not accept any more dust HTLCs, but the channel is otherwise useful.

Speaker 2: 00:17:48

That's one thing as well. You can have virtual queuing or buckets basically.

Speaker 0: 00:17:52

Right, well, everyone already does, right? This doesn't require a change aside from just not force closing. You already won't accept any more DustAge TLCs because you already have a dust exposure limit built out. So that's kind of the easy way to fix this. Just say like, screw it. Like if the peer is being dumb, we'll print errors all the time and just no longer accept DustHTLCs on this channel and max out.

Speaker 3: 00:18:18

Yeah. I was thinking more, we might go for a probabilistic model where your chance of closing is, you haven't got a zero chance of closing. So you can't always be cheated. But your probability of closing over some thing depends on the ratio between how much it's going to cost you and how much you're gonna you're risk losing, right? So, you know, if it's dust, and fees are high, your chance of closing is really low, even though it's it's bogus. And as it approaches one as you start to risk more than you're risking in fees. That that's a bit more a bit less of a here's your free amount here. Here's the if you can easily calculate how much you can cheat I worried that people are gonna start doing that. But the theory is that if you're cheat how?

Speaker 0: 00:19:03

Oh,

Speaker 3: 00:19:03

well, you know, I mean, as you say, you're holding the bag if you close out the HTLC.

Speaker 2: 00:19:07

Yeah, hold the

Speaker 1: 00:19:08

bag. Yeah.

Speaker 2: 00:19:08

Uh huh.

Speaker 0: 00:19:09

Yeah. And

Speaker 3: 00:19:09

obviously, you don't want to control that. So if you just do a problem,

Speaker 2: 00:19:13

yeah, sure.

Speaker 3: 00:19:15

Right. You're kind of going well, you know, at some point, I'm likely to, to close on you. I mean, the theory was that if your peers, if your peers that unresponsive, and you know, it hasn't closed, because it should have, if it's stuck downstream, it should have forced closed so that it could close on you and not get forced closed by you. So if your peers this out to lunch, you're like, how much else is getting through? I wonder.

Speaker 2: 00:19:37

People

Speaker 3: 00:19:37

complain about, oh, you know, you went to change, it's like, hundred sats, but did you really? Or was it just your peers gone to lunch anyway?

Speaker 0: 00:19:45

The other, The other issue that we've seen a bunch of, I think probably the most common forced close reason right now is just, you know, one node forwarded a payment, their peers out to lunch, and they had to force close legitimately, but they don't fail back the inbound edge of that payment until that commitment transaction and HTLC timeout hits the chain. And that those HTLC timeouts were getting delayed long enough that the inbound edge channel was getting force closed. So there's another thing to think about that we intend to have again haven't managed to have a discussion about it yet but like do you just fail it back like you're waiting for something to confirm and like your two options are you fail it back now and just give it up, or your peer is going to force close and presumably their HTLC timeout is going to make it, but now there is that race condition of your HTLC timeout took too long, your peer could technically claim it as a HTLC success, but you're kind of running out of time to fail it back, you need to claim it back. So you have that like really narrow window, but I'm not sure that it's worth the force closed to keep that window open as long as you can. Yeah,

Speaker 3: 00:21:00

no, that's a really good point. At what point do you go, well, you know, it's floating around somewhere, it will get confirmed eventually. We don't have a risk model at the moment for that, and we probably should.

Speaker 0: 00:21:10

Yeah, and I'm not sure that we're even going to bother with a risk model. Like I think, like My personal view is you just fail it back. Like, because you have that narrow window, like you're already screwed. Like your HDLC timeout did not make it by the time you really needed it to make it. We're like, so, you know, we're screwed. Like at least fail it back or something. You're going to lose it anyway. But I think for like just I've been trying to spend a bunch of time with like the plug net people and whatever debugging a bunch of these force closes and I think that is by far the most common force close case today with the like ln debug that's going to get fixed soon hopefully not too far behind it. Yeah,

Speaker 2: 00:21:53

so we had that bug fix. I thought we were doing it, but basically, so what we're seeing sometimes is that like, you know, either the TCP connection is stale or the state machine is stalled. We have a thing where we basically say, okay, well, if we send a state, we don't revert back, revoke in a certain amount of time. We would basically, quote-unquote, tear down the link, which just meant state machine is gone, but the connection is still there. Now, we're going to disconnect. Some people also report that if they just do disconnect-reconnect, they shall see it disappear. Maybe there's some other lingering thing, but at least we can do this to automatically try to, you know, trigger the basically that reconnect, which seems to resolve some things. But for us, it's a super old bug. The bug is like three years, you know, three plus years old and was added, you know, was meant to fix something else. But that seems to be something that people are hitting right now, as far as like those force closures. And, you know, Mempool is chiller now, at least, so maybe it's not as dire. But I'm also, but, you know, I think what Matt was saying there on kind of like the race condition being hit more frequently would explain it, because fees are higher now, and your outgoing doesn't confirm, the incoming doesn't confirm, and then you have a mini cascade basically. But the other thing is, I don't know if anyone's read this paper, something I need to actually take a look at, but it basically hypothesizes kind of like, something I think we've talked about in the past, basically using circular payments to sort of like cancel back an HTLC earlier, which seems related to what we're talking about now with this dash HTLC, like can we use some, you know, hand wave reroute this dash HTLC to somebody else and they'll hold onto it or something like that? I don't really know, but maybe there's something worthwhile there as far as rerouting stuff, which seems to be useful in Azure Web Contacts, but just dropping that. It was an FC earlier this year, but I haven't really caught up with all the papers

Speaker 0: 00:23:26

yet. So does anyone else have thoughts on whether to give up on an HTLC if it's not? I would be very interested to hear thoughts on that, especially the like giving up on an HTLC that is going to result in a backwards force close. That's

Speaker 1: 00:23:44

always what we've been doing in Eclair. When an outgoing HTLC has reached the timeout and we're publishing our HTLC timeout, we are instantly failing back on the incoming link. And we've always been doing that.

Speaker 2: 00:23:58

So you fail back before confirmation. So you're going on-chain outgoing and you fail back as soon as you make that decision basically.

Speaker 1: 00:24:04

Yeah, we take the risk to avoid that chain of getting

Speaker 2: 00:24:08

the in-out

Speaker 1: 00:24:08

and being forced to leave as

Speaker 3: 00:24:10

well. Yeah, we wait for three confirms, which is probably way too conservative, right?

Speaker 2: 00:24:16

Yeah, so we wait for at least a single confirmation. But but I think I think as soon as we go, as soon as we broadcast, we cancel back dust, at least, because you know, things that were just on the outgoing, because they don't really matter anymore. But we don't, we don't do everything, like Claire seems to do.

Speaker 3: 00:24:33

Yeah, so actually, we're the same, because we when we see the transaction, we know which HTLCs are actually in it. Because there's that state where you've got like the two HTLC, the two transactions and you're not sure until you see it on chain. Once you see that mind, we go, okay, cool. These HTCs can close because they're not there. And so, dust goes under that, too. And the ones that are there, then we try to spend it. We wait until it's three deep before we fail back. But even if three or one probably both sucks, right? There probably is a point where we should just fail it back. I'd love to see someone's write-up analysis of like when to do this so I could

Speaker 0: 00:25:14

just type

Speaker 3: 00:25:15

it in and not have to figure it out.

Speaker 2: 00:25:15

That's interesting. Has Eclair always done that or did something happen and y'all decided to start to do that?

Speaker 1: 00:25:21

No, we've always done that. But there's one caveat though, we still wait for the commitment transaction to be confirmed. We don't do that if the HTLC is timed out, but we still haven't confirmed the commitment transaction, we are not failing back those HTLCs yet, because there are potentially two remote commitments and we don't know yet which one is confirmed. But as soon as the one commitment transaction is confirmed, we fail the HTLC timeouts as soon as they timeout on the downstream link.

Speaker 0: 00:25:48

Do you have, I mean you probably don't, you haven't been, we haven't been a high fear rate environment for a while, do you have any sense of how much that, like what the difference is in failure rates across those two models? Because I would assume somewhat naively that there's probably not a huge difference in force closure rates between waiting for the commitment and waiting for the HTLC timeouts.

Speaker 1: 00:26:12

Yeah, I have no idea.

Speaker 0: 00:26:16

Yeah, so anyway, from my, from what I can gather, that is by far the most common force closure today. So I think we're going to reconsider it and I would encourage others to. Yeah,

Speaker 2: 00:26:28

and one thing that we didn't get into 17 or 16 is basically like, um, you know, deadline awareness. So we have a thing right now where we say three comps, it'll always retarget three comps at least, but it doesn't say, oh, there's two blocks now, let me

Speaker 5: 00:26:39

ramp it

Speaker 0: 00:26:40

up. So we

Speaker 2: 00:26:41

had code for that and we try to get it in to fix, you know, other mempool stuff, but I think that'll help a lot. But the main thing is, how much are you willing to pay? Obviously, too, right? Which is where maybe this desk thing factors in somewhat.

Speaker 3: 00:26:55

Yeah, we finally... Sorry.

Speaker 1: 00:26:58

Yeah, that has costed us a bit because we have a deadline aware code that as soon as we get we're getting closer to the deadline we actually RBF more aggressively but we didn't really cap that by the value of the thing we're trying to get back so during the recent hyphy environment it was it cost us much more than what we were actually getting back in the HTLC. So we just changed that.

Speaker 3: 00:27:24

Yeah. So we, as of the release that just came out like this month, we finally are deadline aware better late than never. And we do cap it. And if we've capped it, because you know, it's not worth it. We basically at that point, we ignore it, we basically consider it to be we don't care, we're not going to wait for it anymore. On the theory that it's best effort at that point, because we haven't paid what we think is a fair fee.

Speaker 0: 00:27:51

Which

Speaker 3: 00:27:51

really screws up our accounting. If that happens and it never gets confirmed and we forget the channel and we still have this thing outstanding. Do you mean like once

Speaker 2: 00:28:00

the fee is above that value, you forget that output or something else. So you say we're not even gonna try.

Speaker 3: 00:28:05

So normally our state machine, like we'll wait for every output to be finished and everything else before proceeding. But we won't forget the channel until everything's 100 deep, it's all done. But if we had to lowball something, because it just wasn't that it didn't make sense. So as we approach deadline, we'll ramp up, but we'll never pay more than approximately the value that we're gonna get back, minus some dust, because you know, whatever. And if we've capped it, we mark a flag and say, actually, don't wait for this one. It's not gonna, we don't hold things up anymore for that. So it can end up, but that may never get

Speaker 0: 00:28:39

spent. Gotcha.

Speaker 2: 00:28:46

Cool. Okay. But yeah, I just wanted to get y'all's thoughts around that and just, you know, give them all here. And this is like something that's happening right now. So I have a draft PR and only the report for like the whole just like, you know, hold the bag on dusty shelves. These things, they will just like prototype it more without not committing to it yet. But potentially someone could run the patch themselves if they're really afraid of this conservator or something like that.

Speaker 0: 00:29:08

Yeah, I mean, somewhat related, a bunch of the Pluginet people now are recommending and starting to use the interceptor feature in LND because it does result in the inbound edge being failed back, even if the outbound edge isn't confirmed on chain. So people are manually forcing that behavior, basically. Interesting. Via this hack in LND. I think actually we have the same, you can do the same thing in LDK, or if you intercept an HTLC, we'll force it back maybe. I'm not sure for, anyway.

Speaker 2: 00:29:45

Gotcha, okay, I can see if I got some details or just make sure they're using the right call or something.

Speaker 0: 00:29:50

Cool. Next thing.

Speaker 1: 00:30:00

Taproot and Taproot Gossip. So, all the ideas I get.

Speaker 2: 00:30:05

Yeah, sure. I need to re-re-reload and refresh everything in. We've been doing much review on our side. There's like nine actually PRs. So I have like a bunch of side branches just to make the rebase hell a little bit easier. I think the only one thing popped up in the overview of L&D, basically the way we're trying to like do this non-stuff, you know, basically just to make sure it's like safer using and just like shot changing. And then Eugene brought up something around co-op close that I need to like revisit. Basically the idea was, okay, well, because co-op close, you only ever signing one message, so you only need one non-spare, basically. But there were some things around, and then I added a thing where also it would basically fast accept, right? So as long as what the initiator says is cool, we'll just cut everything short. But I think there was something around them not setting the same free rate and forcing non-true use. But that's one thing I need to take a look at. But if that's the case, then we can just, before we sent another set of nonces after every single reply, basically, we can go back and do it in that if this turns out to be, actually, a thing. So I just need to check that out. But I'm basically just focusing on this just to get everything up and running, get all the tests running there. And then I guess for the gossip stuff, there's just the other things that we need to discuss around how much to go into the 2.0 gradient, basically.

Speaker 6: 00:31:22

No rent.

Speaker 0: 00:31:24

I'm

Speaker 6: 00:31:25

sorry, go ahead.

Speaker 2: 00:31:27

I wasn't gonna say, go ahead, go.

Speaker 6: 00:31:30

I was just gonna say that it's all review on our side right now Got past the hurdle that I was talking about last meeting figure that out and just you know trying to get that in and I Don't know maybe we'll get it in tomorrow I hope

Speaker 0: 00:31:47

and The

Speaker 6: 00:31:50

channel haven't really looked at that yet. So that is really the big issue that since the very beginning of our taproot approach, we kind of have been leading open because I've always had the impression that it was gonna be subject to the most change.

Speaker 2: 00:32:08

Yeah, yeah, it looks like we need to, we just, I guess, so when you say that stuff is ready, does it mean you're all ready for doing interop now?

Speaker 6: 00:32:16

No, we need like probably three PRs to merge for interop.

Speaker 2: 00:32:22

Okay, gotcha. Cool. Okay, That makes sense. I'll post about that clock close thing and just generally refresh all of those other comments. I rebased all the branches last week and just need to kind of start to chip away at all that stuff. Just, you know, I test, test vectors, so forth, weight, estimates. That's kind of like some of the stuff that we're missing right now at least from the specs side of things.

Speaker 6: 00:32:55

Cool. Do we have anything new regarding gossip v1.5 on our side?

Speaker 2: 00:33:04

I think last time there were just questions around like, you know, the whole strictly binding or not binding and then also value amounts.

Speaker 0: 00:33:13

Did you ever start that thread on Lightning Dev? Last I remember you had committed to like do, I mean at this point we might as well just talk about it in New York.

Speaker 2: 00:33:23

Correct, I haven't started it but I'm gonna do it this week. It's on my fucking to-do list. I chopped up a bunch of shit. I imagine we will talk about it in New York as well but at least we can precede some of the discussion and stuff there.

Speaker 0: 00:33:37

And then

Speaker 2: 00:33:37

the one thing we're doing in the background is also just refreshing our tower implementation, just to make sure there's no crazy edge cases for top root stuff as well. That's something I was working on. I don't imagine there's anything there that we haven't thought of, but just to at least cover that base. Because it's a little bit different now. Because you need to send the control block along with it instead of just the signature,

Speaker 0: 00:33:58

et cetera. Cool. What else? Okay.

Speaker 2: 00:34:10

Check, check. We, it looks like, I don't know if I pronounced the word, but quiescence is back on the thing. Yeah,

Speaker 1: 00:34:19

because it's actually a prerequisite for splicing. We started with splicing for now and we finished our splicing implementation and we've been testing it on mainnet to see how it behaves with those high fees and how fun it is to do CPFP and RBF on those. And everything is looking well on the splicing part, but that's also because we hadn't implemented quiescence. We had only done a four-month quiescence where you only do splicing when you have no HTLCs in any commitment whatsoever. So now we are looking at making it with quiescence and with potential HTLCs. So I just think it's time to revive that PR and make sure that it is up to date. But it looks like it has most everything that we need. We just need to spend more time finalizing our implementation.

Speaker 3: 00:35:07

Yeah, it's kind of a weird PR cause it doesn't do anything by itself. You kind of need some other PR to make it useful. Like why would you quiesce unless you've got some other feature? But it does make sense. I think it's a separate feature bit. I think that's the right thing. Yeah, it's pretty easy to implement. You just stop talking when you get the STFU message. And you will go quiescent at some point. The other thing that I wanted to discuss in New York is this idea of the simplified protocol, where we basically just start taking turns. Because it is a subset of the existing protocol, it's pretty easy to implement. And given that we still seem to have lingering state machine bugs, I kind of put it on the back burner cause like who wants to revisit that? But increasingly I might know maybe we do. I know that Greg's L2 implementation actually uses it. So uses the simplified, like rather than being this full duplex thing where they can both have things in flight, this turn-taking thing. And the caveat is that I do not have an implementation of it for LN penalty. And in particular, making sure it's still simple when you reconnect is the main issue.

Speaker 1: 00:36:18

But I wouldn't.

Speaker 0: 00:36:19

Yeah. The good news is, at least my worry that we had that kind of bug with LND was misplaced. And it was not, in fact, simply a state machine bug, but just a hang bug entirely. I'd

Speaker 1: 00:36:35

seen it, I

Speaker 0: 00:36:36

was confused initially because I saw it in a few cases, I saw it with two peers and both of them were in one of these multi things and flight going both directions states. But that didn't turn out to be the issue, luckily. Cool,

Speaker 2: 00:36:50

yeah, and we're looking into one of those with Eclair related to reserve, potentially, where there's a kind of a thing where LND said that they sent something that went below the reserve, but then TVAS had like some very detailed, you know, logs, annotations, and it turns out maybe not. So we're looking into that as well. Not sure exactly what it is, if it's like a rounding thing, or if it's something where, we do do a thing where we basically do a dry run of the full commitment transaction, and then make sure that works, But maybe there's a gap there or something like that.

Speaker 1: 00:37:18

But it really doesn't have anything to do with the reserve. That is the bug I reported because if you look at it, if you look at the state, it just does nothing that makes sense as a reserve issue. It's nothing adds up to make it look like it's a reserve issue because the HTLC that's an issue here is an HTLC sent by LND. So LND decides to send this and it doesn't make it fall into below its reserve. So I really think it's unrelated. People think that it's linked to reserve, but I don't think this is really the issue. So there's a theoretical bug. Some inconsistency.

Speaker 3: 00:37:55

There's a theoretical bug we've always had because when we decided to go full async, If you have a fee change in flight, it's not completely determined whether or not an HTLC is valid. Depends on what state that's in, which is really fucking annoying. One of the things that the flight update commitment says is that you can only change fees by itself, right? So Because your turn taking now, when it's your turn, you can change fees or you can add HTLCs. You can't do both. Which means you both know exactly the state you're in and it's very deterministic what happens, right? Either you can afford the fees or you can't and what you do if you can't. Whereas at the moment you can end up with a case of, oh, but I've added these HTLCs, but now on the new fee rate, I can't afford them, but we're kind of fucked. And that's a problem we've always had. That's kind of annoyed me. And we're all hoping that update fee goes away entirely, but until then it's possible that you could hit something like that if fee rates... Yeah,

Speaker 1: 00:38:49

but you just hope we get rid of update fee before we hit those bugs. At least in the case that I was investigating, I had the user put the whole Eclat channel state so I could see exactly what changes were proposed and signed and act. And there was no update fee in flight in either direction. So because that was my intuition as well, that it had to do with something like that. But actually, the state was much simpler. So I really don't see what went wrong and what caused the LendD to send internal error.

Speaker 2: 00:39:19

Yeah, it sounds like a inconsistency. Cause like, you know, we do, we have like two or three levels of checks. One is the dry run. The other is like reporting what's available. Maybe it's something where it's like a user sent out a payment that skipped the normal forwarding path at the same time that we did the forwarding or something like that. But I think you gave extremely detailed analysis. I think we can just look into that and plug it in, maybe try to reproduce it and see what's happening there and make sure we can prior

Speaker 0: 00:39:44

that.

Speaker 5: 00:39:51

Yeah,

Speaker 3: 00:39:51

we already have code. Yeah, sorry. Go ahead. We already have code to avoid sending two update fees back to back. We will wait till one's completely settled before sending another one just because it seemed to trigger people's state machines when that happened.

Speaker 2: 00:40:04

So yes, so we're fixing a bug for that. Yeah, we're basically like, we have we have a PR and it's that we seem to get it basically like if you if the channel starts and you be in you know, you said funding locked and then immediately send an update fee, we'll lose the update fee because we haven't finished processing the channel yet basically. That one is known and we have a fix for that as well. One of those race conditioning things where it's like a message is propagating, but at the same time, we don't have a thing spun up yet. So we'll just not even learn that you sent that. And then anything that we send after that will be invalid because we didn't process that. But yeah, so, and I think along the way, we're also gonna fix that really old issue around like, what is it? Like update feed and funding lock. There's some ordering thing where we send them on order because they say, but that's going to be fixed as well alongside that too. So. Cool.

Speaker 0: 00:40:52

Nice. And

Speaker 1: 00:40:56

regarding the option simplified commitment, apart from the fact that we're taking turns, everyone, everything else just stays the same. We exchange the same messages, commit, seek, revoke, and act,

Speaker 3: 00:41:05

but just different P or, okay. Yep. We add two messages. One is a yield. So you can basically go, oh, your turn. And the other is a no-op add. So you can provoke a yield. If you want to have a really naive implementation, rather than replaying, if you want a turn, you just send the no up. And that just invokes a yield. Otherwise, you just start sending updates. And either you get back an update, in which case it's there. Because if it's your turn, I can either just send an update and you just ignore it and send your own updates, in which case you're taking your return because that could happen simultaneously, right? Otherwise you send a yield to say, yes, no, no, it's your turn after you, sir. And then you go. So it's basically just two messages and the no-op message is trivial. But it does allow for really naive implementation. Obviously, if you're employing both at the moment, you don't get the advantage of it. But at some point in the future, when everyone supports it, you could have a very, very naive, very simple state machine. That's way simpler than what we do now. And in practice, in theory, if you've got low, you don't lose any latency on a partially low utilization channel, because you're taking turns anyway. You do, in theory, lose some latency on a high use channel, but in practice it just means that you're batching a little bit more, which is probably closer to optimal anyway. So it actually doesn't have many disadvantages.

Speaker 0: 00:42:29

Cool.

Speaker 1: 00:42:34

I don't have anything else on quiescence and splicing apart from the next PR I opened just a one-line PR about the HCLC receiver and channel reserve it's something that I ran into because of splicing because splicing make it makes this thing obvious But I think you all remember that almost three years ago we discovered that channels could get stuck when all the funds were on the non-initiator side. And because when you add an HTLC, it increases the fee of a commitment transaction. So you must make sure that the initiator who is receiving that HTLC will not dip into its channel reserve and will not dip below zero, basically. But actually, the spec says that as a sender, you must make sure that the receiver of the HTLC, when he's the initiator, is able to pay the increased fee, but also needs to maintain its channel reserve. Otherwise you don't send an HTLC. I don't think there's any good reason to make them maintain the channel reserve because they just need to be able to pay that fee and they are actually receiving an HTLC. So if it fails back and it took them below their channel reserve, if HTLC fails, they will go back to above the channel reserves. And if the FTSE fulfills, they get even more funds. So it makes the channel balance more balanced. Because the issue with placing is that when you're in a state where the non-initiator with the fund, the initiator is slightly above their reserve, but the non-initiator swaps in some funds. The reserve is here because it's 1% of now a bigger channel. So now you are stuck. Now you are in this situation when you, the non-initiator has all the funds of the channel but they're not about any HCLCs so you're really screwed. But this can actually happen also without splicing so I was looking at fixing this and I was wondering what implementations do on the receiver side. If you are the receiver and receive an HCLC you did use the increase fee but do you also verify that you maintain your channel balance? And if so, do you just let the HCLC go or do you force close? Maybe I can share the one line in a... In a clear...

Speaker 2: 00:44:48

Yeah, basically what you're saying is that like adding funds basically can cause a reserve requirement to go up. And if that goes up, maybe you can cause something to become stuck even outside of like that slight edge case, right?

Speaker 1: 00:44:59

What's interesting is that the spec says that the sender should not send such HTLCs, but there's no requirement on the receiver. So if no one implemented the receiver side requirement, we can just drop the sender side requirements and get rid of this issue. Otherwise, we have to do to be a bit more to think about it some more if we want to make it backwards compatible. So I'm curious what implementations do on the receiver side. If you are the initiator, you receive an HTLC, you compute the increased commit fee, you verify that it's lower than your balance but do you also verify that you still meet your channel reserve after that increase in the commit TXV?

Speaker 2: 00:45:41

For LNB I think yes and it'll result in like us like sending an error or a warning and just kind of stopping. I need to check with biscuit, but the, the full scenario is like, what do you do as a, as a receiver if you're the initiator, and you're about to propose a CLC, or you receive a Chelsea that causes you to dip below the reserve or you receive an update fee that causes that?

Speaker 0: 00:46:06

Yeah. All right.

Speaker 1: 00:46:09

Because actually, I don't see why we need to maintain the reserve in that case. We only need the initiator to still be able to pay the fee for the commitment transaction, but we don't really care if it makes them fall below the reserve.

Speaker 2: 00:46:27

And this is what the receiver who is the initiator does, right? So as the initiator, you receive an HTLC, you display the reserve, what do you do?

Speaker 1: 00:46:35

Yeah, exactly.

Speaker 0: 00:46:37

Do

Speaker 1: 00:46:37

you even check it? Interesting. Because the spec doesn't say that as a receiver, you have to check it. We do check it in Eclair, and I'm removing that, but I wanted to know if other implementations are checking that as well. Because in the cloud, that would result in a false close, which is bad. But if no one else does that, we can still phase it out pretty quickly.

Speaker 0: 00:46:59

But

Speaker 1: 00:46:59

if other people do that, because we'll have to fix that for spacing anyway because with spacing since the since the reserve is set to 1% and 1% of something that grows becomes bigger we will hit that issue a lot more so we have to figure out a way to fix it but maybe we can just say that for channels that have the splice feature, then we do it differently and we skip that check and we allow dipping into the channel reserve in that case. But it would be easier if we don't have to add an if to the specification and we just do that in all cases. Yeah,

Speaker 3: 00:47:30

it looks like... I think L

Speaker 2: 00:47:32

and B were error out. I

Speaker 3: 00:47:34

think so. I think we did. Yeah, so

Speaker 2: 00:47:37

we'll take that and then basically run a simulation to see what it would look like and then I think we error out. And I think at that point the channel was kind of stuck if you sent a sit, because you retransmit that on restart basically. But looking at it right now, I've looked at this code a while, so I think we'll error out basically.

Speaker 0: 00:48:01

Do

Speaker 1: 00:48:01

you agree that conceptually we should not, there's no reason. So the other guy is making you dip into your channel reserve, but that's because they're adding an HCLC to you. So that will just improve the situation and you have no incentive to actually reject that channel. You're dipping into your channel reserve, but yeah, that's okay.

Speaker 2: 00:48:20

Yeah, and because we might gain funds if that thing sells. But I guess like, but can this be allowed to happen multiple times? Because eventually won't we go to zero? But I guess you're saying that, like, well, you're letting us go to zero.

Speaker 1: 00:48:31

Yeah, you're still checking that you cannot go below zero because then you cannot create the commitment transaction and pay the right fee. So that would fail if you go below zero, but I think you can go all the way to zero.

Speaker 2: 00:48:43

Yeah, and I think we actually hit this. Some people were reporting that like they were doing winter like rebalancing stuff and they inadvertently got their channel into the state because they were just you know doing winter rebalancing in a loop.

Speaker 0: 00:48:54

So what

Speaker 3: 00:48:55

you're saying is in theory there's no reason for me to check you're not pushing me under my reserve because that's your problem not mine. Okay. And

Speaker 1: 00:49:03

what's nice is that if we had implemented that, we could remove the sender requirement as well. Because the sender wants to push the funds. Because otherwise, the channel is stuck. And they have a lot of money on their side. They want to be able to make payments. They don't really care if you go below your reserve because it's only temporary and it will either if the HTLC fails, we get back to the situation we had before. If it succeeds, there's more funds on the other side. So it means they meet their channel reserve. So it's a win win situation to remove that. But we will probably have a backwards compatibility issue.

Speaker 2: 00:49:38

Yeah, I think the logic there follows. And this is just, I think just us trying to be super defensive after we had, you know, weird stuff happen in the past in this area.

Speaker 1: 00:49:49

We need to look at that last thing also.

Speaker 0: 00:49:52

But

Speaker 1: 00:49:52

then maybe something we can do is just cheap on the implementation side removing the check on the receiver, and the senders will still not send those HTLCs, but the receivers will not check it anymore. And so that when we can see that everyone, at least enough people have updated, then we can remove the sender requirements as well. Yeah, and

Speaker 2: 00:50:12

I guess you can just assume that is there for splicing. I just want to mention that explicitly, I guess. Because yeah, I can see how this can happen more. Yeah, more in terms of.

Speaker 0: 00:50:26

All right,

Speaker 1: 00:50:26

so I think I'll detail that more directly on that PR. We've planned to only remove the, Because since the specification doesn't really match what the implementations do, and the implementations are currently more defensive than the specification, I'll just make it a hint for implementers to drop a receiver side requirement, ship something, and then six months later we will drop from the spec both all the requirements and from the implementations as well. And hopefully that will match the timeline for specing as well.

Speaker 0: 00:50:57

Yeah. Perfect.

Speaker 1: 00:51:04

So next we have, is there anything new on attributable errors? I don't think Yost is here. No, he's not here.

Speaker 2: 00:51:13

Nothing new.

Speaker 1: 00:51:20

And the channel reestablished requirements are not a priority either. Your storage backup, anything new? Nope.

Speaker 0: 00:51:31

All right. So,

Speaker 1: 00:51:34

is there... So, I need to double check. Has there been progress on issue 934 that was opened a while ago where you should not directly publish your commitment when you receive an outdated channel reestablished, but instead wait for an error. Did everyone ship that in the implementation? I didn't check. Good question. I think we haven't shipped this yet. I think us and

Speaker 2: 00:52:02

CL Lightning or CL had like a little thing that was patched over basically, just because I think they wait a bit of time now, but we do have this opportunity. We do have this thing coming up to fix the whole, let's disconnect, if they get an error. So we can potentially slip it in there. My thing just then was just making sure that like it aligned with expectations of older LND nodes kind of a thing.

Speaker 3: 00:52:26

Yeah, that that shim is still to do actually we released without it. But yeah, the problem is that we would get upset with them. We would send a warning on error, which means you don't close the channel. But then we would hang up on them. And we would usually not receive their error in time. And so it means you get in this state where often they had to manually force close the channel rather than happening automatically in the case where their peer has just lost their shit. So the answer is to wait a little, I mean, the workaround is to wait a little bit. The bigger thing is to change it so that we don't automatically

Speaker 0: 00:53:03

drop

Speaker 3: 00:53:03

the connection when there's a warning set. But I worry about the side effects of that too. So that's a little bit more invasive change.

Speaker 5: 00:53:14

Okay. Yeah.

Speaker 2: 00:53:19

I mean, hey, at least for us, we know people are on some newer versions as a result of stuff that happened, so there's that. The word is just to make sure we're not breaking what's old stuff. One other thing that I know is out of you all, remember we did the whole thing where we basically made max-hclc required, right? And as we were reporting that they were doing syncs and then basically they were rejecting old updates just because they didn't have the field set. And I think we did it and I think there's a bunch of new ones that aren't being sent, but maybe there's some older node that has these legacy ones. But I told people, hey, just don't sweat that, basically. But I didn't realize that, initial graph analytics, we basically, people get that stuff. But it's something that's interesting.

Speaker 0: 00:54:01

Yeah,

Speaker 3: 00:54:01

it's not real. So those those those ones we looked at, we looked at this, we went, what the hell, we were sending it out, like a warning at that point, hang up, because they're sending us this bad gossip. We dropped that we started just ignoring them. But It was really interesting. So we looked at the ones they are, and they're like five years old. So they're sending us really ancient channel updates for like why? So that was the problem. We didn't find any recent channel updates like in the last six months or something that would actually be valid that they should ever be sending us that have this issue. So the answer is just to ignore them. Yeah.

Speaker 2: 00:54:39

Okay. Yeah. Just some viewers like, you know, people would just freak out about logs. So maybe we can make like a warning or something instead of like error or whatever. Yeah.

Speaker 3: 00:54:49

Yeah. Or check the timestamp. If the timestamp is too far in the past, then just drop it. But if it's recent and they don't have that field, then yeah, something's weird. But I don't think you'll

Speaker 2: 00:55:00

Yeah. It's probably one of the cases where like we would have accepted it, but then pruned it because it was a zombie, like the next block or something. Yeah, exactly.

Speaker 0: 00:55:09

Okay, cool.

Speaker 3: 00:55:13

Yeah, definitely older than two weeks.

Speaker 1: 00:55:16

Yeah,

Speaker 2: 00:55:16

yeah.

Speaker 0: 00:55:18

Yeah,

Speaker 2: 00:55:18

and we know, you know, no one needs are sending that stuff out. Also, don't do some custom. Cool. Um,

Speaker 1: 00:55:27

I see that Val wanted to talk about subject.

Speaker 2: 00:55:31

Oh yeah.

Speaker 1: 00:55:32

Regarding LSP stuff.

Speaker 4: 00:55:34

Yeah, I realized no one's probably read this yet, but basically we have some LSPs that want to be able to take an extra fee on the first payment that an end user receives to cover the inbound channel opening fee. So basically all it does is it just forwards less than the onion says. And there's an extra TLV in the update add HTLC that says how much fee they took. And we also have a configuration option, obviously. So you just have to opt into it on a per channel basis, although that's not part of the spec. So yeah, it's pretty early, but we're hoping to get this in pretty soon. So if there's any initial thoughts on that, that'd be great.

Speaker 1: 00:56:17

But does that mean that whenever you receive an HTLC that you should forward to a client and doesn't have enough capacity, you're going to immediately open a channel and forward that HTLC with those additional TLVs?

Speaker 4: 00:56:30

I was mostly just thinking about the first channel that they receive because I think they're gonna Potentially include some buffer so that they won't just open at just enough to cover the initial payment only

Speaker 1: 00:56:42

I really think that that that cannot Work well because of MPP because as VLSP potentially you receive a first HLC to forward but then there are two others that are coming. So what we're doing in Phoenix is that we have this small protocol where when we receive an HCLC and there's not enough balance, we send a new message that's called a pay to open request. We just include the onion and we let the client receive it as if it were an HLC, they could be on the on, see that there's potentially more coming because there's a total amount. So we let the client aggregate everything. We also have those pending on the LSP side. So when the client has everything, they can send a message to say, oh, then please open a channel. I need to receive all of those. And then we negotiate the fee. How

Speaker 4: 00:57:27

does the fee negotiation happen on that?

Speaker 1: 00:57:30

Oh, it's really, it's the LSP says, I want to take that much fee and the client has to say, OK or not OK. So the way we do that is that we are changing the way it's going to be done in Phoenix, where the user will be able to control that liquidity, not liquidity allocation, but how much they are willing to pay with both max percentage and max flat amount. So the user just sets this once and it will automatically accept or reject depending on what VLSP says.

Speaker 4: 00:58:04

Got it. Okay. I mean, is that incompatible with this? Because wouldn't they just do all that calculation and then just set the fee to whatever was previously agreed upon or negotiated? Oh,

Speaker 1: 00:58:14

yeah, that's not incompatible at all. But it's just that your proposal alone seems that it's only part of the thing. And if you only do that naively, you're going to be opening too many channels and wasting a lot of on-chain fees.

Speaker 2: 00:58:28

Can you repeat the thing around the MPP incompatibility there, Thibas, that it works, but if you have many HTLCs, then...

Speaker 1: 00:58:36

Or is that... Yeah, it's that. As VLSP, if someone is sending an MPP payment that's split into, let's say, three HTLCs, for example, you receive the first one as VLSP, You have a channel to that mobile wallet, there's enough liquidity, you forward a normal HTLC. Then there's another HTLC coming with that same payment hash, but now there's not enough balance to forward it on the channel. So you send another message with the onion. If you instantly open a channel and forward that HCLC, then when the third HCLC comes in, seconds after that, you're gonna open yet another channel and forward it on that one, which is really sad. I see what you're saying. You forward the onions to let the client tell you, okay, I received everything, that's the amount, open the channel for just that amount, and then forward the ACLs through that.

Speaker 2: 00:59:22

Makes sense. Basically, y'all have MPP awareness at the LSP node to make sure you can give it back a single unit to avoid the

Speaker 1: 00:59:29

slow trickle. Okay, that makes a lot of sense. And on the mobile wallet side, because the important part is that it's the mobile wallet that still receives the onions and that does the aggregation. And only at the end of the aggregation says, okay, I'm ready to receive everything. Please open a channel for that amount. Right. But

Speaker 3: 00:59:46

it's got to be something other than add HTLC, right? Because you can't add those HTLCs to the channel yet, right?

Speaker 1: 00:59:51

Yeah, that's why we have a new message, a new lighting message that contains the onion and some information about the, I think, the amount, something that really looks like add HTLC, but is not tied to a channel. Yeah. Yeah,

Speaker 4: 01:00:05

that makes sense. And I think C equals was discussing that. This is just kind of what they requested, so we're kind of just rolling with it. But I think we should talk about that in more detail. But you're kind of saying that there should be some additional parts to this, basically.

Speaker 1: 01:00:22

Yeah, I think it's better to really look at the whole thing before doing small parts because otherwise it's going to be hard to have something that really works end-to-end. And see, we haven't yet made that into a blip or even a bolt because we figured it could be directly, at least the TLVs added to existing messages could be directly in the bolts because it will eventually benefit everyone to be able to run the same kind of LSP things. But we really think that splicing and liquidity ads are really important tools for an LSP. So we think we have to do those first before we do any kind of LSP specification.

Speaker 0: 01:01:00

Got

Speaker 4: 01:01:00

it. That makes sense. OK, I'll Bring that back and see what the LSPs that we're talking to think about that, but that makes perfect sense to me. So cool

Speaker 0: 01:01:11

That's interesting

Speaker 2: 01:01:16

Cool guess that's about it. I think we have two more of these before the Spring meeting. Maybe one, depending on time zones, do something again. Cool. I just posted that and then also Carla had that. She made an issue tracking kind of like discussion topics like we've done in the past. I haven't done anything there yet, but you should have to check that out so we can start to like look at what the schedule looks like. She posted it there.

Speaker 1: 01:01:46

It's going to be fun. Oh, yeah.

Speaker 2: 01:01:49

Yeah, New York.

Speaker 0: 01:01:51

The funny

Speaker 2: 01:01:52

thing is I was looking at like, you know, Airbnb and stuff. It's like, yeah, New York is expensive. And that area itself is, you know, the most expensive. I was like, okay.

Speaker 0: 01:02:00

But you

Speaker 2: 01:02:01

know, I think I can crash my friends' ways or something. Cool. Cool. Okay. See people on chat and stuff them. And then, you know,

Speaker 0: 01:02:16

I'll.
