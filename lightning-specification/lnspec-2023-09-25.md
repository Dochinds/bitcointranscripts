---
title: Lightning Specification Meeting
transcript_by: carlaKC via TBTBTC v1.0.0
tags: ['lightning']
date: 2023-09-25
---

Speaker 0: 00:00:00

Yeah, so you've got to right. So yeah, Bastian pointed out that

Speaker 1: 00:00:04

we did not do this, and we have to do this. So I've been working on the code so that you have to push their commitment transaction. Now, there are several problems with this one is that there are two in many states are two possible commitment transactions they could have. In theory, you know what they are in practice, it turned out to be a bit of a nightmare to unwind the state. So I actually just ended up just saving the commitment transactions as we sign off on them. And then you just keep the last two. Of course, with splicing, you can have multiple in flight making it even more fun. But potentially, you could be in two states if they haven't revoked the previous. So there's a pile of fun to be had and dealing with that. And even once you've done that, you've got the problem that you can only tell about your mempool, you can't affect other people's mempool. So really this is screaming for package relay or-

Speaker 2: 00:00:56

Honestly, just sign everything you can and then just send it straight to the miners and then.

Speaker 1: 00:01:03

Well, yeah. Matt, yeah.

Speaker 3: 00:01:09

You shouldn't ever need to do this if you assume package relay. And even if you don't assume package relay, you still don't need to do this because you're not going to get into the mempool anyway, if the thing doesn't have high Fee.

Speaker 1: 00:01:23

No, so so so they're, they're, yeah, their commitment transactions, the mempool, and they're not interested in pushing it, you need it. So you need to push on their anchor, right? That's why they're two.

Speaker 4: 00:01:34

You can,

Speaker 3: 00:01:36

you can replace. Oh, yeah. I mean, you need, yeah, again, you need package relay.

Speaker 1: 00:01:40

Yeah. So without package relay, you should at least, which we didn't do, we pushed our own anchor, which our transaction, we put a transaction, it's not going in, let's spend the anchor and push and child pays for parent it. But what we didn't do is the case, of course, where the reason we can't even get in our mempool because their commitment transactions in the mempool and we need to push their one. And because we don't look in the mempool, it's a little bit fun. No, it's not a PR that's discussed now. It was it was a bug in core lightning that well, a hole in core lightning that we did not cover. So I've gone through and having the fun of implementing that.

Speaker 3: 00:02:17

Half the time your commitment transaction doesn't make it into the mempool just because it doesn't have enough fee anyway. Well there's that too right?

Speaker 5: 00:02:26

Yeah.

Speaker 3: 00:02:28

There's just no game around it. Until then everything's fucked. Why are you trying?

Speaker 1: 00:02:33

Yeah, this? Yeah, we're papering over a little bit. Yeah, because that hates you. Well, I mean, statistically, there's there's a decent chance that it's their commitment transaction, the mempool not yours. So yeah, but yeah, it still isn't solved in general.

Speaker 3: 00:02:51

I had some channel I couldn't close this weekend that was anchored but had zero, or maybe a week or two ago that had zero, or just didn't have enough fee, two or three separate bytes or something. No, maybe I had five because it eventually confirmed after the fee rate limits came down. But in the meantime, like, Alikay kept trying to bump the fee and kept trying to bump the fee and kept trying to bump the fee. Had I not caught it, it would have spent a lot of money on fees.

Speaker 1: 00:03:21

Yeah, we fortunately do have a limit that we will not spend more than we could ever get. Like basically some of the HTLCs are timed out and certain things We go, we're gonna push that. But yeah, it's still be a lot of stats you throw.

Speaker 3: 00:03:36

Yeah, we talked about that. I wasn't a huge fan of that, using that limit. I mean, that is the only limit because indeed you can still blow a lot of money.

Speaker 1: 00:03:47

Yeah, absolutely. We had a fun one a while ago where we would, if we were offline and came back, which is not unusual, and so we forced to close the channel, we would then freak out and go, wow, it's been like, you know, 30 blocks, nothing's happening. And we would just ramp up the fee, or fee bump. Ridiculously. So yeah, because we had originally no limit on the theory that we could be in a bidding war. And we should spend all the sats. Yeah. Yeah. Oh, man. So it turns out building on Bitcoin is hard.

Speaker 3: 00:04:27

Yeah, we need package reliance. That's all our problems. So we're done. Not all.

Speaker 1: 00:04:33

Not all our problems. Not all our problems. Yeah, a decent, a decent number just vanish. This is what I think this was my original justification for not doing this thing where you spend their anchor. But meanwhile, I'll put it in a big banner saying package relay package relay package relay. So I know where to delete.

Speaker 6: 00:05:00

And it's not for this, I mean, to spend your other guy on Cardboard, because you might have two very convenient transactions, and you might have like one version in your main pool and the other version in the rest of the fragment tools.

Speaker 1: 00:05:13

Yeah, exactly. It's not enough. You know, you could pay for it with onion messages where you send me an onion message and I will broadcast that transaction for you, which is not probably, you know, not the worst idea anyway. But at some point you're building, you know, a package relay. Badly. Is Bastien here today? You're building

Speaker 5: 00:05:40

back a tree like Badly

Speaker 3: 00:05:40

It's best in here today Someone was saying and I receive that he will not be joining. Oh, okay. Vincenzo was saying in IRC that that's not gonna happen today.

Speaker 1: 00:06:02

Oh yes, sorry, he did send me a message saying he wasn't going to make this spec meeting. Okay. All right. And the same message that he told me, by the way, you have this thing and

Speaker 5: 00:06:16

you need to write all this code today.

Speaker 1: 00:06:21

Okay. Cool. Okay. Should

Speaker 3: 00:06:26

we start working our way through?

Speaker 0: 00:06:30

11.08.

Speaker 1: 00:06:32

Modern feature bit assignment.

Speaker 0: 00:06:32

1108,

Speaker 1: 00:06:32

modern feature bit assignment. So yeah, this is basically a practice that we came up with for splicing that we thought was generally probably a good recommendation. That while features influx, you just add a hundred to the feature bit and use that as a stand in. If nothing changes with the final implementation, it's easy for you to have an implementation that accepts both, but it avoids the problem that you change the spec and then you end up with these broken, older nodes out there and confusion and bad things. So it seems pretty well act. The only question, T-Bass asked whether we should also do this for TLVs. Maybe. I think it's almost a step too far. I mean, how much code do you really wanna change?

Speaker 7: 00:07:32

Yeah, my knee-jerk reaction is probably not.

Speaker 1: 00:07:37

Yeah, I mean, you might choose that for some particular thing, but in general.

Speaker 3: 00:07:41

I mean, the same considerations apply.

Speaker 1: 00:07:47

They kind of do, except that at this point, if you really want to, you can do the, oh, you're using the old feature bit. We will interpret it this way. It is a bit messier.

Speaker 3: 00:07:56

You could, but in a lot of cases, we just deal with the message. If the TLV is set, we don't bother looking at the feature bit because why would we look at the feature bit?

Speaker 1: 00:08:05

That's right.

Speaker 3: 00:08:07

And so you kind of do have the same issue there.

Speaker 1: 00:08:13

Yeah, for us in our code base, it'd be a lot messier to do the TLB versions than the feature bit. The feature bit's really easy for us. I mean, the theory is that by the time you get to deploying it, you're kind of close to what it'll finally look like, you'd hope. So I'm not as convinced.

Speaker 3: 00:08:39

We could also just suggest if you need to change the feature be changed to feature, the TLV type, you change the TLV type, and then we've quote unquote wasted a TLV type in a message. But we just recycle it eventually, because hopefully, it won't turn into experimental garbage forever.

Speaker 7: 00:08:56

I mean, TLV types are also message specific. So we can shred through those pretty aggressively.

Speaker 1: 00:09:02

That's true. Yeah. OK. But I think everyone's happy with at least this is it stands. So shall we merge? Yeah. Cool. Looks like everyone's active, so I'm going to hit the merge button.

Speaker 3: 00:09:19

Okay

Speaker 1: 00:09:22

Right, that's easy Okay 1109 another clarification So we use these terms, we didn't define them.

Speaker 5: 00:09:42

Okay.

Speaker 1: 00:09:46

So we talked about features being offered and features being negotiated. But the only subtlety is that if you said it was compulsory, then you can basically assume that it's negotiated, because it's up to the other peer to hang up if they did not understand it. I.e., you're allowed to just avoid it. Just set it to compulsory and just forget it and never check it. But I guess that's the only subtlety in that. But yeah, we use these terms in the spec. We just never actually said what offered means and what negotiated means, even though I think they're pretty clear. But, Bastian again had a comment.

Speaker 7: 00:10:30

Yeah. Just the less brain cells we have to use to infer what the term means, the better, I think.

Speaker 1: 00:10:36

That's what I figured. So T-Best made a comment. I'm not sure I understand it.

Speaker 5: 00:11:05

Okay. I

Speaker 1: 00:11:05

think I'm going to have to go around on this again, because I do not understand T Beth's comment. So it used to be that L and D would check, would care about the, would check that the peer accepted the feature of it, but that's not in the spec. You're supposed to check what they offer you, and they're supposed to hang up if you offer them something they can't accept. Now for most features currently, it's symmetrical anyway, so it doesn't matter. But in theory, I can offer you a feature that you don't offer me, and that's OK. So you're supposed to basically just hang up on the other one. Particular things like for pure backups, for example, there's separate bits. There's one bit to say, hey, I want you to back me up. And there's another bit that says, I offer backup services. And they're not necessarily symmetric. You can say, you must back me up, but I don't have to offer that bit, for example. So the spec is written. I mean, you're allowed to hang up on anyone for any reason. But the spec is written as in you read their feature bits. You look through if there are any unknown ones that you don't like, you hang up. But yeah, so I will push that back.

Speaker 7: 00:12:20

Rusty, is there a convention then about, is it always a service that you're offering that a feature bit is? Or is like a request for you to offer when you take your own feature bit?

Speaker 1: 00:12:30

That's a very good question. It's a little bit meta. Man, I need another coffee to answer that question, I think. Generally, feature bits at the moment are the symmetrical requirement for us to both offer some feature, but they're not technically. Ah, see that's, yeah, the difference between offer and supports. Supports is not actionable. Offers is like you set the bet. That's very clear. Supports is a much more vague statement about do you support something? Do you understand it? Offers is very clear. Offer is you set it in the bit. Whether you support it or not, it feels more meta to me. That's why I like the word offer. Offer means you set it in the bit, and set it in the init message, or in some contexts in the node announcement. Whereas supports is, I mean, you can support something that you do not offer, for example. Right? That's very different.

Speaker 7: 00:13:42

Yeah, offering implies support. But not the other way around.

Speaker 1: 00:13:47

Yeah, that's right. I mean, we can support a feature that we don't offer to anyone else. But if they offer it, we will handle it, perhaps.

Speaker 3: 00:13:56

So

Speaker 1: 00:13:58

OK. Max HLC value in flight,

Speaker 0: 00:14:04

1113.

Speaker 4: 00:14:07

Sorry, I had a meeting hangover.

Speaker 5: 00:14:12

Cool.

Speaker 1: 00:14:15

I have not looked at this, so I'm glad to get it now.

Speaker 7: 00:14:19

It's just a wording clarification on the semantics of max HTLC value in flight. I just was kind of digging through the spec for some implementation reasons and found that it wasn't quite specific about whether or not it was the sum total of both sides of the channel or like both halves of the the htlc buckets or if it was just one and so I just

Speaker 5: 00:14:43

try

Speaker 7: 00:14:43

to clarify it

Speaker 1: 00:14:45

I don't think you can restrict the total. I think you can only restrict what they send you.

Speaker 4: 00:14:57

Yeah, it's one of those things that's sort of like asymmetric, where it's like I set a value and you set a value. And the value that I set is restricting you and the other way around. Wait, let me check the text a little bit.

Speaker 1: 00:15:07

Yeah, I think that's right. Because you can both offer at the same time, you could accidentally step over any limit that was supposed to do the combined thing. So it is literally, Yeah, you tell me, don't send me more than this.

Speaker 4: 00:15:24

Yeah, okay. I guess, do you think the critical change here is on the bottom, Keegan? Like the sum total versus total value?

Speaker 7: 00:15:38

This doesn't actually change the semantics as Rusty noted. I don't think the, it was just a point of confusion as I was reading the spec and I tried to reword it in a way that was like less, if I had encountered the wording that I put down, I would have not had confusion. Maybe people disagree with me, maybe they agree. I tried to get- I

Speaker 3: 00:15:56

think the point is the first topic doesn't say anything about offered. It just says total exposure to HTLCs.

Speaker 1: 00:16:03

Yeah, which is true. It does allow you to limit your exposure, but it isn't sufficient to limit your exposure. All right, it's assuming that you also limit your own amount you sent. But how many more words do we want to add to point that out? Yeah. Cool. I mean, that really, for me, goes under the spelling rule. So we can just

Speaker 4: 00:16:29

kind of

Speaker 5: 00:16:30

hack it.

Speaker 4: 00:16:31

Yeah, it does have Morehouse and TBaaS. Thumbs up on it.

Speaker 1: 00:16:35

Yep. Okay. I'm going to say yes and apply.

Speaker 5: 00:16:39

Cool. Cool. Okay. Harmonize. CLTB Explorer.

Speaker 1: 00:16:51

Where did we get with that? What was the magic number today?

Speaker 0: 00:16:56

2016,

Speaker 6: 00:16:57

something like this. Like I think it's already come on B2B Foundation, just want to clarify and get spec for them.

Speaker 4: 00:17:04

Yeah, I think before it said 3,000 or something like that. And then we said that we all did 2016. It looks to be in line with that now. So.

Speaker 5: 00:17:15

Cool.

Speaker 4: 00:17:16

Yeah, looks good to me.

Speaker 1: 00:17:19

Yeah. I'm not sure. Yeah, we argued over the deserves own section or whether you should just literally put 2016 in that point.

Speaker 6: 00:17:25

Yeah. Yeah. Yeah. Yeah. You know, it's cleaner. It's simpler to have like a one-night change but add on the other end like Apple to clear from something like you can reuse it for a I'm breaking like you mental speckles and now I can refound this, or, like, channel date extension, or that kind of things. So.

Speaker 1: 00:17:46

Yeah, I know, It's kind of bulky for, I know, it's annoying to like take everything and reduce it down to a one line change. But the problem is the indirection, you're reading the spec and then you're like, Oh, this max value, where's this max value? You look over there and you go, oh, you mean 2016. Okay, that would have been good to know. Like it's a lot of words to say. It's not defined by default. It says defined by default. No, no, it's not defined by default. It's defined as, right? It's like literally there's no default. This is the value.

Speaker 6: 00:18:16

Okay, fine, all right, yeah, I can change it.

Speaker 1: 00:18:18

You can't change it. It was literally

Speaker 0: 00:18:21

2016.

Speaker 3: 00:18:22

I mean, you could

Speaker 1: 00:18:24

give it a name. You could say like max CLT brackets

Speaker 0: 00:18:28

2016.

Speaker 1: 00:18:30

Right, so you can point at it. But yeah, just tell us 2016 and we'll all do it. But I mean, we'll all do nothing.

Speaker 4: 00:18:39

It also looks like it just tries to modify the x3 too soon portion as well, which doesn't look correct to me at the glance. Before it was like, if it's too close, but now you're trying to like factor max HLC into it being too close. I'm looking at line one, two, three in the diff on the left-hand side.

Speaker 1: 00:18:56

Is it supposed to be the next bit where it says, or talks about being too far in the future?

Speaker 4: 00:19:02

I think that would not be a thing in terms of moving. I think we should be modifying the line below.

Speaker 6: 00:19:12

I think roast beef is correct. I mean It should be the next one.

Speaker 4: 00:19:17

Yeah. Yeah, just bump it down one. OK, cool. I got onto that just so you have a place

Speaker 6: 00:19:21

over there. We don't have like our like it's same like everyone has on like a custom value for unreasonable in the other presence.

Speaker 4: 00:19:31

Yeah, I think we all just sort of pick something for that one too. I don't know what ours is off head. Maybe it's like 20 blocks something. I don't remember exactly. In terms of when we reject for being too close. So, that's another round.

Speaker 6: 00:19:47

Well, I will modify this proposal at least. On the right.

Speaker 1: 00:19:56

Okay, cool. Cool. But I think that was good. Okay.

Speaker 0: 00:20:08

1096.

Speaker 1: 00:20:08

So, there's just been some light progress on this. This is the simplified close. T-Bas had a whole heap of feedback showing that I can't type or spell. And there was one fix where the case where I don't give you your signature because yours would be dust. But T-Best said he is trying to implement that now. So I'm gonna let him do that and give us feedback and tell us which bits broke. I tried to really spell it out, like do this, do this, in this case, which in some ways makes it less clear to think about, but much easier to implement. Because it's really broken down to this multiple cases.

Speaker 4: 00:21:02

Gotcha. Yeah, this is still on my list to start to work through an implementation on. We're almost done with our current LND.17 release, and I can have some more bandwidth to take a look at this more deeply.

Speaker 1: 00:21:12

Yeah. So yeah, so hopefully we'll get some motion on that. I mean, I don't feel hugely wedded to this. I kind of made it up as we go. So if it turns out to be impossible to implement, then we'll fix it. Okay, Yeah, I mean, it enshrines the dust rule, which was mentioned previously. And basically adds op return. Cool. Spec cleanup 1092. Now we've decided to kind of shelve this for the moment, basically make them compulsory and then later on see if we can start ignoring them. We have in our next release in November, we will be making all these features compulsory. So that'll be a nice test balloon. Some of them already are of our onions, already compulsory for us anyway. And we will be weaning off the old anchors. We no longer offer it, but if someone else offers it, we will still accept it. And we still support channels that are that. We're going to do the beta upgrade thing, because you can only ever do that if you enable experimental features. And if you like experimental features, you'll love experimental upgrade. So our idea is that we would upgrade to zero fee anchors, because that turns out to always be possible to upgrade, because you're just basically reducing the fees you're paying. So there's no state you can be in where you can't just upgrade from the old non zero fee anchors to the zero fee. So it's actually like the most trivial case. So we will be implementing that and then we can rip out support for non-Zero-Fe anchors. And then we can change the spec just to call those anchors and pretend the other ones don't exist.

Speaker 4: 00:23:15

Don't you need some additional like synchronization there? Because like the, the SIG has flags are different now, right? And so now you need

Speaker 3: 00:23:22

to be able to verify.

Speaker 1: 00:23:24

Yeah.

Speaker 4: 00:23:25

Or no, I guess we're the same and we just move the fees to zero. Just thinking about like, you know, education, like retransmission or something like that.

Speaker 1: 00:23:33

Yeah, so we still need to use the STFU kind of the quiescence thing where, what happens? For the upgrade proposal thing, you basically when you reconnect, you say, hey, I want to be this state. And if you're actually got nothing in flight, The other side goes, yes, now we're the state and you're in the new channel type. We did it previously for static remote key. We could upgrade static remote key like as an experimental thing. And that case we kept in our database at what points we changed, which is actually kind of dumb. The only reason you care is if you ever see one on chain, you're like, oh wait, hold on, this is old style. But we didn't need to do that. You really don't need to do it for anchor app. You look at what's the, you know, cause you grind the, We grind the fee anyway. We just grind the fee. If you go, oh, it's zero. Okay, I know what you're doing. This was a zero fee anchor. So I don't even think we need to add anything to the database to remember when we changed. We can just change and our on-chain code should just be able to intuit it from what it sees. That's my theory.

Speaker 4: 00:24:32

Yeah, Have you added anything to the STFU message? Or is it just sort of like, you know to send that, and it just knows because the version it's on and things like that? Or are you threading through any additional CLP, context, or anything like that?

Speaker 1: 00:24:43

Technically, STFU is... Power. Technically, STFU is optional. So the way the protocol works is when you reconnect, you say, I want to upgrade the channel. And if there's no retransmissions, then the other side acts that and says, and you go, yes, we're good. We've upgraded the channel.

Speaker 4: 00:25:00

Uh-huh. I guess what message are you sending to say, I want to upgrade? Is that STFU or is that something else?

Speaker 1: 00:25:05

No, that's in the reestablish. The TLB in the reestablish says, hey, I want to be this kind of channel. And the other side goes, cool, I'm

Speaker 3: 00:25:11

happy with that.

Speaker 1: 00:25:13

But it can only work if there's no retransmissions going on. If there are retransmissions going on, then the upgrade fails and you will reconnect at some point. You can use the SDFU to force this case, but statistically, just, you know, you're fine. What, you know, your chances that every time you reconnect... So imagine like perhaps for one or two versions, we'll support this case where we still handle the old code and we'll do an upgrade opportunistically. And given that they had to be experimental in the first place and turn the shit on, I'm just going to go with, they're not going to get unlucky and always have an HTLC in flight that they need to retransmit at the time they reconnect. And probably we'll have closed. If they fall all the way through that version, then the next version will probably force close those channels or something. And no one will notice it. That's my theory.

Speaker 5: 00:26:06

Anyway, that

Speaker 4: 00:26:06

makes sense. Actually, because like, you know, Keegan starts to look more into kind of like the whole channel upgrade thing as well. So I was wondering if like, there are just kind of like, you know, what, what like the simple, simple case. I think you have like a very simple case of like, there's just zero fees. So there's not too, you know, there's no like new, you know, pub keys to exchange or anything like that. So interesting.

Speaker 1: 00:26:23

I

Speaker 4: 00:26:23

was trying to like look at this like, you know, holistically.

Speaker 1: 00:26:26

Yeah, cause we did it for static remote key and that was similar that you can just pretty much upgrade.

Speaker 3: 00:26:32

So yeah, anything where you don't have to-

Speaker 1: 00:26:34

Yeah, the no param upgrades are the easy case. Yeah. And yeah, so we accidentally made another one.

Speaker 4: 00:26:41

Cool.

Speaker 1: 00:26:42

Offers. I don't have any progress. There's, I've, on my to-do list is to upgrade to the latest spec. The addition is that you can now have an introduction point to the blinded path that is a short channel ID and a direction bit. Using the pubkey hack, where 01, 00, and 01 become a magic direction value. It's spec'd, it's in the spec, but we haven't actually implemented it, but we will, we'll implement it. We won't ever create such a thing except in dev mode, but we will accept them so that if others do.

Speaker 3: 00:27:20

Cool.

Speaker 1: 00:27:21

Ah, there. We're in the quiescence protocol, so this is the STFU thing. I don't think there's anything.

Speaker 4: 00:27:30

I don't think there's anything. I think he was just looking at it to like better understand, you know, the uses and when you like officially want to start to enforce it. I know it has some splicing overlap as well, but I don't think it's used there, but something that I think like the final version would incorporate some.

Speaker 1: 00:27:44

Yeah, so splicing is does use FTFU and there's a comment here saying

Speaker 5: 00:27:50

Yep.

Speaker 1: 00:27:53

I think leads into what you were gonna say.

Speaker 7: 00:27:55

Yeah, I would love to be able to use this as part of the dynamic commitments work just to like simplify the number of different, like you said, in the rationale for having this at all, it's like, it can be nice to have just a primitive in the protocol to like get everything to stop so that we can do whatever else we wanna do. The only thing that I saw in this that I was like scratching my head about a little bit, was that because the requirements for this double synchronization of commitment transactions, I think it is possible, but probably statistically unlikely that you can get into a flow of messages and heavy channel traffic where you can't actually get the channel to SDFU. I'm not 100% sure because I'm still like kind of getting up to speed on how pending updates apply, but I've listed out a sequence of things that if you loop through them, that it would just never allow the thing to STFU. And maybe that's OK. But it could be beneficial to have a two-phase sort of thing where it's like, OK, block the adding of updates and then actually commit to the STFU.

Speaker 1: 00:29:11

Yeah, because the requirement is that you don't send it if there's anything pending for either peer, which means I think if you've got a flood of traffic, you will never... I mean I think you should

Speaker 4: 00:29:29

have shut down like semantics where after you send it, you're supposed to stop sending stuff?

Speaker 1: 00:29:34

That's the thing, but there's also a gate on sending it in the first place, which may be overzealous.

Speaker 7: 00:29:39

I was like, you could do a two phase version of this where

Speaker 4: 00:29:42

it's like,

Speaker 7: 00:29:44

yes, there are pending updates, but like stop. And then there's the one that's like, okay, there are no more pending updates. Now we're like really stopped.

Speaker 4: 00:29:51

So like double active kind of, yeah.

Speaker 1: 00:29:54

Yeah, so like shutdown where you only reply when it is all clear.

Speaker 4: 00:30:00

Yeah, closing sign implies that.

Speaker 1: 00:30:03

I think, yeah, I think that would work. Yes, the receiver is not supposed to send any more updates. But the sender is restricted on when they can send it. And that's the issue. It's been a long time since I've looked at this. So I think there's probably a reason why you don't, why it works this way. But yeah, okay. I will, I'll put that on my to-do list today to figure out if it's simple or major to fix it up. You're right, I think statistically it works, but we should make it more robust.

Speaker 5: 00:30:54

Cool.

Speaker 1: 00:30:59

Cool. Is there anything new on splicing dynamic commitments, upgrade and reestablish? Otherwise we go to Taproot because I'm sure there's exciting stuff there.

Speaker 4: 00:31:09

I think for splicing, I saw Dusty commented some like a laundry list. I think there was some like minor chatter on IRC, but I think people just know like the final things to start to fill in. So I got that.

Speaker 1: 00:31:22

I noticed that the spec is mainly lies. What he's implemented is not.

Speaker 3: 00:31:26

Like I basically went

Speaker 1: 00:31:27

to re-import, I went, oh, let's re-import the CSVs from the spec. And I went, hold, these don't match what you've actually got here, Dusty. You, yeah. Yeah, some things changed and they didn't get written down. So they're on the list. Now, hopefully everything is in that list that I made. If there isn't, please tell me to add it. Cool. That's cool.

Speaker 2: 00:31:49

On Taproot, LDK, we're prioritizing 1.17 before we merge Taproot stuff. So nothing really exciting to report from our end.

Speaker 4: 00:32:01

What's 1.17? Is that like a LDK PR number? Our

Speaker 5: 00:32:04

next release.

Speaker 4: 00:32:04

Oh, right. Next release. Gotcha. Oh, yeah. It's a

Speaker 5: 00:32:08

high number, I guess.

Speaker 4: 00:32:09

Oh yeah. PR number.

Speaker 1: 00:32:10

Up in the hundreds.

Speaker 2: 00:32:11

For reasons that we're not allowed to disclose publicly. Oh, We're really desperately trying to get it in this week.

Speaker 4: 00:32:19

I see, I see. Okay, cool. Nothing, you know, too big change. We've had some more testing and things like that. People were testing like recovery, SEB, stuff like that. We found like some small things with like, you know, certain API calls, like not being fully updated on our side. I still haven't added the test vectors yet as well. It's still kind of in that state that I had it on prior. And I think we have a better way to, that we have another project to have the test vectors along with some unit tests to make it a little bit easier. But that's sort of where we are with that. I think probably the next week or so, we can start to pick up the gossip changes more now. But now that I will be shipped over to other stuff, I think we'll have to start to look at that and work through some of the feedback in the PR and then just see how things line up on the code side of stuff.

Speaker 1: 00:33:08

Cool, yeah, I noticed on the gossip PR, there's like a, I'm gonna get to this real soon marker, which I'm happy with because that means I don't have to do anything again.

Speaker 4: 00:33:17

Yeah, yeah, yeah. So he's working on some other stuff. Cool. But I guess, all right, so you're saying working on getting the next release out and then Yellowback basically focused, or back to pick up the patch and wrap, right?

Speaker 8: 00:33:28

Yeah.

Speaker 5: 00:33:29

OK, cool.

Speaker 8: 00:33:31

By the way, with regard to the Taproot gossip, I'm

Speaker 9: 00:33:35

not sure, I don't think I'm seeing Al here, but how final is the spec?

Speaker 4: 00:33:43

Not. So, we haven't started code at all. I think the spec represents a state after a conversation in New York, basically around combining the messages, being able to advertise both, the whole like, you know, hand outness and music too thing and so forth. So like, I wouldn't say it's super final by that regard, and there's no code at all committed to it. And I think everyone needs to take another look at it after to make sure it matches what we thought we had in our minds, you know, leaving New York.

Speaker 8: 00:34:10

Yeah, okay, cool, thanks.

Speaker 4: 00:34:12

Yeah, I mean, if you wanna dive in and leave a bunch of comments, there's like, no one's gonna get mad because they feel like it's almost done.

Speaker 9: 00:34:19

Oh yeah, that's well, I have to dive into my code first and yeah, after Antrop I'll be happy to leave gossip comments.

Speaker 4: 00:34:29

Cool, Yeah, the main thing we're looking at is just like the messages and just having that line up with like, you know, expectations and requirements and stuff like that. And that's sort of working right now. Tomorrow recollecting.

Speaker 1: 00:34:41

Yeah, the gossip stuff for us is critical, because that's the first thing we're going to implement. So we make sure that we can see other people's taproot channels, which is like a minimum viable, right? So as people start publishing them, at least we can use them.

Speaker 2: 00:34:55

Yeah, and I think the other

Speaker 4: 00:34:55

thing about that too, is like, trying to always like, we're supporting advertising the old with the new formats, that also like helps bridge the network sooner versus like a hard switch over and there's no one or don't we talk to whatever so that's good

Speaker 9: 00:35:07

I don't want to couch our horn too much but one funny side effect of RGS is that with all of the data that was being stripped out and principally could advertise taproot channels or include taproot channels and then put it snapshots there without nodes being any the wiser that those are taproot channels. Because they don't need to know they're just routing through them.

Speaker 4: 00:35:29

Yeah, Good point. I guess it depends on how much validation you do. Without it, if you have validated knowledge, just insert. Sure. Because otherwise, like, you know, maybe they don't know how to like parse or pay the taproot or something like that. But yeah, but I mean, you're totally right. If you have some side server gossip thing, you can do whatever. To the client is okay with that.

Speaker 9: 00:35:54

I'm nodding.

Speaker 1: 00:36:05

Attributable errors, has anything happened?

Speaker 4: 00:36:10

Not much. I think we're starting to take another look at the PRs now that like we're down, we're down to level 17, our upcoming major release. Last I remember, Tobias was looking at it, or Ben Warteg was looking at it, and there was something around reducing the size of the HMAC, something, something. But I think now it'll be picked back up as far as review and stuff like that. But I don't think anything super actionable yet.

Speaker 1: 00:36:36

Cool. And channel reestablish requirements. There are two of these PRs.

Speaker 4: 00:36:47

Yes, this is something very old in my to-do. I think we realized that like, you know, this part of the... I can't remember why there's two of them. I think one of them, 1051, I think is like a bigger overhaul to like some of the wording.

Speaker 3: 00:37:04

Yeah.

Speaker 4: 00:37:04

And I think the other one was meant to just be like clarification. Yeah. Around like failing with, oh yeah, I think this is meant to be around failing the channel versus sending the error first, that whole thing.

Speaker 1: 00:37:18

Yeah, okay.

Speaker 4: 00:37:20

So for now, it's the user review.

Speaker 1: 00:37:24

Yeah, homework everyone should check out these, including me. There's a big banner at the top that says, T-Bass has requested your review, so I'm guessing that I should review it. Cool.

Speaker 3: 00:37:35

Yeah, I'd like that, I think. Yeah.

Speaker 1: 00:37:38

Right. I think, that marks everything that we had in seeking review.

Speaker 4: 00:37:50

Yeah, there's one thing that I mentioned on chat is like, so like, remember a while back, we had these like gossip extensions. I think, you know, Eclair wanted like the timestamp one and the check someone, I thought like, you know, it wasn't super necessary basically. So we never implemented it. But now I realized that like, we have like an issue where like, if a node is either like off on her long period of time, or has poor connectivity, they'll never sort of like resurrect zombie channels, right? So what this is, what's it like certain LND nodes are like missing portions of the graph, either because the new update won't propagate to them or other unknown reasons, right? And the reason why that matters is like, you know, for example, if we mark something as a zombie, we wait until we see a fresh update to actually like resurrect it basically. And if that never comes or due to the ordering, if we're down for a while, we've proven before other stuff, then we have these gaps. So I think we're looking into implementing the timestamp extension basically for the Gossip Queries. So we can basically see that, oh, there's a newer timestamp than what we have right now. Maybe this thing actually isn't a zombie. And maybe there's like a few thousand channels that are in this state. You know, we tested as far as like, you know, wiping this like, you know, sort of like zombie cats that we have and we're seeking and things like that. So that's one thing we're looking into because it can affect pathfinding. Obviously, you don't have portions of the graph. There can never be a path sometimes, but I was wondering if people implemented that, The timestamp thing already, in terms of like us finding peers and stuff. Yeah, when we

Speaker 3: 00:39:08

start, because of the, basically because of this issue, we never bothered to implement, or like we implemented gossip queries and then we immediately ripped it out and never actually use it because of this issue. We just always download the full graph every time we start up because LND didn't implement the timestamp thing.

Speaker 5: 00:39:27

Gotcha.

Speaker 4: 00:39:28

Okay. Okay, that's good to know.

Speaker 1: 00:39:31

I'm looking forward to Gossip v 1.5 for this. Taproot Gossip makes it easier for us to sync stuff. But yeah.

Speaker 3: 00:39:39

Yeah, I think we'll probably just, we won't bother. We'll just go straight to MiniSketch and we'll just keep downloading the full graph on startup. It doesn't take very long. It doesn't cost anything really.

Speaker 4: 00:39:50

Gotcha. Yeah, because I remember like the thing and the reason why we add this initially like there was a while back where like testnet had like this thing where like it would just turn zombies over and over again and testnet was boring other than this. That's why we started to like add some of these protections there. But you're like, it's not super large. And you know, particularly if you're already using the RGS thing, then you get a pretty streamlined dump of it. But there's something that we realized that like can affect particular in particular mobile phones, because no neutrino nodes don't, you know, they don't always download all the blocks, things like that. And they also may not always see channel closes the way like a full node will as well. So we had some stuff to work around there. But OK, cool. So at least we can get it from clear nodes and then to lighting nodes. And then we'll start to serve that data as well. That's like a second background thing that I realized, like, you know, people were just complaining, why do I have 20K channels less? I was like, oh, you know, they're not really needed. But like, you know, those are actually the free channels.

Speaker 1: 00:40:41

Yeah, I'd be tempted to put a heuristic in to go, we expect this many channels. And if we were way short of that, we should start just scraping stuff. You know, we try to do stuff where if we see an update for a channel we don't know about, we start to get suspicious and we do probes and stuff, but we don't have an absolute threshold. Other than for a fresh node, we have nothing in gossip, then we will reach out and grab stuff. It's all heuristics and bullshit. It really is not pretty.

Speaker 3: 00:41:08

Just sync the full graph on startup when you make your first three peers you connect to, sync the full graph. It works great. Don't have to think

Speaker 1: 00:41:15

about it. Well, I

Speaker 4: 00:41:17

mean, so I think it's like, so we actually do sync the full graph. But I mean, and so what we do is every 20 minutes or so, we'll basically do a diff on our channel IDs and y'all's channel IDs. The issue is that we see channel ID 5, and we're like, that's a zombie. But we're not going to fetch that thing. To avoid downloading it and then pruning it right after. So, so that's why we don't always just like get everything. We treat the zombie edges slightly differently, but now the gap in our logic was okay, well, you know, you may already have the edge and it may be a zombie, but there could be a newer update. And that's where we're trying to be able to fill the gap on there.

Speaker 3: 00:41:50

Right, because yeah, you have to actually have the timestamp to know. But you might as well just download it. It's not very much data.

Speaker 1: 00:42:02

Yeah, but you end up churning zombies. We had zombie protection code that we ripped out because it wasn't working correctly and we never put it back. So we still have the issue that we will unzombie things as updates come in from one side. So yeah, I'm looking at reworking that for this release.

Speaker 3: 00:42:24

Yeah, I think we will un-zombie it, but we won't actually use it when routing, so it doesn't really matter. Delete it again later.

Speaker 1: 00:42:32

They tend not to be critical stuff. But if your node's been down for a long time, then that may

Speaker 5: 00:42:38

mess with things as well.

Speaker 4: 00:42:39

Yeah, and usually it's like, oh, my node was down for a week, and I didn't notice, and I came back up. And then I have 10k less channels or something like that.

Speaker 1: 00:42:48

Yeah, we run a prune timer that runs every day. So you come up, you've got like a day to sync. We won't prune immediately at least.

Speaker 5: 00:42:59

So

Speaker 0: 00:42:59

you've got some

Speaker 1: 00:42:59

window, But you could still get, this could still get messy.

Speaker 5: 00:43:04

Yeah,

Speaker 4: 00:43:04

yeah, we should. Because I think part of the issue is like, recruiting is the first thing we do, but we should, like you're saying, have like some time to give them chance, give them time to like propagate and shit.

Speaker 1: 00:43:13

Yeah, because we found that we were throwing away our whole, all our gossip the first time we came back.

Speaker 4: 00:43:18

It's like, oh, you've been down for a month? Everything's a zombie.

Speaker 1: 00:43:21

That's right. Yeah, it's all gone. You know, why can't I pay anything? Oh, yeah. Yeah. Cool. Okay. Right. Is there anything else that we should talk about? There were some weird people who were complaining about force closes on mainnet, bolts in particular, I don't know about others.

Speaker 4: 00:43:47

I didn't see. Yeah, I had an issue. We fixed an issue of this on-chain thing. So it was a case where it's like, if we were using a hodl invoice, which I'm pretty sure there were, and like hand wave concurrency, nasty stuff, it could deadlock, but we fixed that one in particular. And there was this other like peer-based one, basically, like another kind of like an academic thing. One thing I'm realizing as well is that like, for example, we had some random ones for our nodes as well. And we realized like many times it was just due to like the worst case of a cascading force close where no one could actually get into the chain. And it's a combination of no anchors, or even if you have anchors, not bumping aggressively enough, or not being aware of the deadlines coming up. So it's one of those things where it's like, because the men pool was flooded with people doing this dollar auction type stuff, basically, people just couldn't get in time, basically. So we're looking at reprioritizing, because we'll at least target a deadline, but we won't do the bump on a curve type of a thing. And the other thing that relates to that is also like, the things that we talked about around dust HDLCs, where right now, we'll unfortunately go to chain for a dust HDLC and we won't cancel back until the outgoing is fully resolved. But if it's a dust, you can number one, maybe just not go to chain at all, but then also cancel back a lot earlier. So things around dust HDLCs leading to timeouts, leading to congestion, leading to that whole spiral is what I think is going to happen. At least for the cascade. It would pass a few weeks.

Speaker 1: 00:45:11

Yeah, we put some logic. So I know, it Claire has logic where it basically will immediately close as soon as it goes to chain. Like they take the chance. We give it more and we kind of go, hold on, we're in danger of our peer force closing on us. Huh, the outgoing one is on chain, hasn't been resolved yet. Fuck it, we're just gonna take the chance that is not now gonna be resolved rather than lose the channel behind us. So we try to cut through the cascade. If that happens for any reason, and it could be because it's a dust HTLC, it could be because congestion, it could be because of bugs, whatever it is, we now try to break that. Because it's weird, because it's the only case where you care about your peers' timeout. Normally that's their responsibility. Like if they have a HTLC that's gonna timeout, that's on them. But this is the one case where you go, huh, well, hold on, I suspect they're going to hang up on me soon. So I'm gonna have to close it. So we've we've taken that chance. Because I think people would prefer that to a force close.

Speaker 4: 00:46:16

Yeah, yeah. We have anything to force close.

Speaker 3: 00:46:17

And we will probably do it, but we have not yet done it.

Speaker 1: 00:46:21

The other thing is the tolerance. Is that if you have a whole thing

Speaker 4: 00:46:23

early or not going on chain back?

Speaker 3: 00:46:27

Both. You go on chain and then you fail back early.

Speaker 4: 00:46:31

Gotcha.

Speaker 1: 00:46:32

Yeah. The question of tolerance, right? So is there some level of HLC which you know, obviously, if it's a dust HLC going on chain doesn't win you much, except for like, you know, vengeance, or kind of getting upset with channels that are not working well. But yeah, the idea of having some statistical tolerance is something I've kind of toyed with. I mean, rather than have some absolute cutoff, have some kind of statistical thing where you go, let's flip a coin or do a curve and go, No, I'm not going to close the channel just for this one. Particularly when you combine it with the cascading logic, where you close out the HTLC, even though the upstream's still stuck. Maybe somewhere we have to go eventually, because people really hate forced closes. Now, maybe they'll hate losing money more. I don't know.

Speaker 2: 00:47:20

Yeah. I mean, I think

Speaker 4: 00:47:21

that's the interesting thing about choosing to not go on chain is that best case, something happens, it gets canceled back. Worst case, you're just eating this thing basically, and You need to keep track of your debt, basically, to pay it down because it starts to choke into your channel capacity. But I think the one related thing, I think we talked about in the past, but that is like, I think something Antoine started a while back is also like having a different desk limit. So there's a different limit for max desk HTLC that doesn't necessarily hamper your throughput. So you can keep that on. And it's basically two different buckets. And they're very different classes.

Speaker 1: 00:47:55

Yeah. So someone should spec that, because it'd be interesting to see what it looks like.

Speaker 4: 00:48:01

I think there's a PR from the initial death saga we went through. It's just kind of sitting there perhaps. Or we talked about making it. But yeah, those are the two different causes of buckets.

Speaker 3: 00:48:15

I have been seeing some evidence of peers just losing HTLC is like I had an LNDP or force closed a little bit ago. Sadly, he lost his logs. But I did note that none of his the HTLC was routed through his node, and none of his other channels, He had a number of other channels force closed I think due to the same issue but none of them had the HTLC that was forwarded through his node.

Speaker 4: 00:48:42

And it wasn't dust?

Speaker 3: 00:48:43

And it was not dust it wasn't close to it was like a hundred case hats or something. It could have been dust on one of his other channels, but I think that's unlikely.

Speaker 4: 00:48:51

Yeah, it seems unlikely.

Speaker 3: 00:48:52

So I don't, I sadly don't have logs, so I'm not gonna like open an issue, but I've seen some weak ads.

Speaker 4: 00:48:59

Yeah, because one thing- There

Speaker 3: 00:49:00

may still be an HTLC hang somewhere.

Speaker 4: 00:49:02

Yeah, one thing we did fix in 17 was the old thing where like if you like sent a channel update super quickly after the reestablish like we would guarantee to like not really handle that but now that's handled as far as like synchronization there. I think that's also fixed some other channel update things. At least that's one thing that we know is an issue. But right now, the main thing that we know is an issue is this desktop is the thing we're looking to have some interim thing. Probably just do the cancer back early, and then examine the not going to chain with some of the other implications that that adds.

Speaker 1: 00:49:36

SPEAKER 1 Speaking of channel updates, So what do people do with channel updates that receive an error message? We put it in our gossip store, but that's potentially an information leak. So we'll start broadcasting out to everyone else. Which is always a bit me.

Speaker 4: 00:49:54

I think I think yeah, so we take it verify apply it. But I'm not sure we broadcast it to others.

Speaker 3: 00:50:01

We don't broadcast it. But if somebody else like does a full fetch from us, then it would be included in that.

Speaker 5: 00:50:09

We'll see. But if

Speaker 4: 00:50:11

you take the information leak, the rest of you are watching to see if you got my update kind of thing?

Speaker 3: 00:50:15

Yeah. They're watching. Ah. Yeah. I mean, we could do

Speaker 1: 00:50:21

some kind of dandelion thing. You I mean, you can apply it locally just for this payment and then forget it. Which would work right. So you don't call even Carly and cross other payments. You could do we do some kind of dandelion thing where you spread it to a peer and then, you know, eventually it explodes and, you know, something like that. Although if you only got one peer, it's pretty obvious at that point. Maybe you don't do it in that case. I just wondered what others did. Cause we will put it in our Gossip store, which will validate everything else. And if all comes good, we basically just do it, go back and start again and do a path find now that it's in there. But that's a little bit too social. We also have potentially a problem with Gossip V 1.5 that Christian pointed out where, because you use block heights and you can't have two updates with the same block height. If you go, you take the first one, it's easy to segment the network and do a similar kind of thing where you basically pollute the network with injecting gossip and go, and then you can tell by when someone makes a payment, which update they had for your channel. You must be in this region of the graph because I pushed that into there. So the rules around how you overlap gossip, if you get two gossip at the same timestamp are tricky. In particular, if you everyone goes mini sketch and everything else, you may never see both of them because you may think that they're the same thing. So there's there's actually an open issue like fish on the

Speaker 4: 00:51:49

Yeah, I think for the high thing, you know, there definitely needs to be some like burst tolerance, whatever that is, if that needs to be global or synchronized, I'm not sure, but yeah, you definitely need some like burst tolerance to number and handle those retries and then also the propagate stuff.

Speaker 1: 00:52:03

Yeah, I mean, you... Sorry, Matt. No, I was

Speaker 3: 00:52:09

just gonna say, it seems like we really kind of have to either like ignore it completely, do a pathfinding that just avoids that channel, or which sucks, or do a dandelion thing, I mean, and also, or like, you know, you do a route that just, you fetch a new route using that information, but you don't, you know, use it for future payments, you just use it for this payment, you don't edit the gossip store. And you just wait. And then all future payments will also fail or you have to segment it completely. Like I don't think anything that works at the peer to peer level is going to work short of like a dandelion thing.

Speaker 1: 00:52:48

Yeah, yeah, no, I'm tempted to say you should just apply it locally for this payment, and then hope that it propagates, you know, in a genuine way. If they're working properly, it'll propagate in a genuine way, and you'll get, you're just a bit behind, that's fine. If they're trying to do weird shit where they advertise one thing and then when everyone asks they send something else You're gonna get bouncing off them and that's their problem. Really You're gonna read you're gonna hit them every time If there's a little best route and maybe you bias against them at that point. I don't know But I just feel our implementation is probably wrong and leaking information.

Speaker 4: 00:53:21

Yeah, that's that's a really good point. I'll check what we do. I feel like maybe we like queue it to get sent out eventually, but like on a delayed basis. But this is a good point. I was like, I like some analogies of like transaction broadcasting stuff like that.

Speaker 5: 00:53:34

I don't

Speaker 1: 00:53:34

think it's the biggest information leak we have in the network, but I thought I'd ask.

Speaker 3: 00:53:40

Sounds pretty nasty. I disagree with that, actually. It's one of the worst ones.

Speaker 4: 00:53:49

I mean, it sounds very inadvertent.

Speaker 1: 00:53:52

It's not because no one's doing it now, Matt. Because no one's talking about it.

Speaker 4: 00:53:58

I didn't write this down.

Speaker 3: 00:54:00

It's fine. It's not a security issue. It's not being exploited.

Speaker 1: 00:54:04

That's right. Exactly. I don't, I've looked at

Speaker 3: 00:54:08

the issue and citizen lab points it out that it's been used against them. You know? Yeah.

Speaker 4: 00:54:16

Okay. All right. I'll, I'll check what we do and maybe modify it, maybe don't, maybe we do the right thing. Maybe there's nothing to check.

Speaker 3: 00:54:26

Cool.

Speaker 1: 00:54:29

Okay, I'll put it on my to-do as well. Cool, okay. Is there anything else we should, is burning on fire on the network? No,

Speaker 5: 00:54:45

that's about it. Cool. Cool.

Speaker 4: 00:54:48

Okay. All right. See

Speaker 1: 00:54:58

y'all on chatting stuff. I've got

Speaker 5: 00:55:00

my latest to do list. Yeah. Always. All right.

Speaker 4: 00:55:01

Yeah. Peace out. Thanks.

Speaker 5: 00:55:02

Yeah.

Speaker 3: 00:55:03

See you.

Speaker 1: 00:55:04

Bye folks
