---
title: Lightning Specification Meeting
transcript_by: carlaKC via TBTBTC v1.0.0
tags: ['lightning']
date: 2023-06-05
---

Speaker 0: 00:00:05

All right, so let's start. So the first PR on the list is one we already discussed last week. It just needs someone from another team, either Lightning Labs or LDK to hack. It's just a clarification on bolt 8. So I'm not sure, I don't think it should be reviewed right now because just takes time to work through it and verify that it matches your implementation, but if someone on either of those teams can add it to their to-do list, hack it, and then we can merge it. Then the next one for which we have discussions happening, a lot of discussions happening, is allowing an HTLC receiver to dip into the channel reserve, 1083. So this is kind of reviving the issue we had with stuck channels a while ago, because when the initiator had a balance that was getting dangerously close to its reserve. It was not able to actually add new HTLCs to the commitment transaction because it put deep into the reserve and channels were blocked with all of the liquidity on the non-initiator side. And this was an issue. We added a buffer to make sure that this doesn't happen realistically. In practice it works quite well but splicing adds, make us go back into that situation because with splicing the initiator can end up being below the new reserve since the reserve is a dynamic is 1% of the total channel size. So when the non-initiator places some funds in, then you can end up with the initiator being below their reserve. And you still want the non-initiator to be able to send HTLCs so that the initiator goes back towards meeting its reserve eventually. So how should we do that? My proposal was to, on the sender side, when you're not the initiator, to just below sending that HTLC, because you are actually letting the initiator dip into its channel reserve but it's actually already dipped into its channel reserve because of a splice. If you only send one outgoing HTLC and wait for it to resolve, you control how much extra risk you're taking. But it is still an extra risk it's more yeah it depends on the on the way you look at it in the specification this is the sentence I'm changing is more general than that and there was a lot of good feedback from Matt Morehouse saying that instead of doing that, we could just, if it's only for spacing that it's really happening, and it's true that in practice, it's only for spacing that this is really gonna happen, we can just keep tracking the old reserve until the new one is met, which is a bit more tracking work, but I'm fine with that as well. If people really don't want to add something to the spec that gets the sender deep into the channel reserve. So I don't know if Matt, you had time to look at my latest comment.

Speaker 1: 00:03:01

I didn't, but I'm still a little confused by this because what you're going to set it as far as I understand what you're in essence doing is just increasing that buffer. By, you know, one HTLC or whatever you're not actually solving the problem because there's still cases where someone can run a bunch of HTLCs at once, like if both sides push 400 at once, you still have the same problem. Either that or you just let them go like all the way down.

Speaker 0: 00:03:25

No, that cannot happen because the initiator is still not allowed to dip into that reserve. So the initiator cannot add new HTLCs once they reach the reserve, but you're only allowing the non-initiator.

Speaker 1: 00:03:38

But the whole problem with this was that things happen at the same time, right? So both nodes send 400 HTLCs at the same time. The initiator or the initiator sends some number of HTLCs to put the non-initiator right at their channel reserve. And then the non-initiator at the same time sends, you know, the same number of HTLCs to dip into the channel reserve by that number of HTLCs. And I just don't think we can, like, you haven't actually solved the problem unless you want to let them put 400 HTLCs worth of fees out of the channel reserve, which I don't think we should do.

Speaker 0: 00:04:16

OK. Yes, so the issue, what you say in a sense is that when the non-initiator cannot know that they're in the situation where they should add only one HTLC because they are taking extra risk because The initiator is concurrently adding other HTLCs, which increases the size of the transaction. Okay, that makes sense.

Speaker 1: 00:04:35

The whole issue is concurrency. If we want to solve the- so I didn't- I, I missed the context. This was also for splicing and I agree with Matt Morehouse that we should solve the splicing case by just tracking it. If we're worried about the race condition still, which you know I don't have any numbers on how often it actually happens and we should get numbers, we're worried about it, but I think we should just do the old RESTU proposal and just do updates one direction at a time.

Speaker 0: 00:05:01

Okay I think we'll eventually do that but I think we will get splicing before that. So I want a solution that happens before we actually rework the commitment update scheme. So yes, I guess I'll go then with closing that PR and just adding to the SPLICE spec PR one node that you should keep tracking the old reserve requirement until the new one is met and that I think that should solve it. All right then that sounds good to me and it sounds like it's what Matt Morehouse asked as well. So that should be Okay, so I'll just take it to the Splice spec PR So next step is CLTV handling in Vignit Pass Yeah, this one is

Speaker 1: 00:05:49

I still haven't done anything. I was traveling a bunch the last week. I promise

Speaker 0: 00:05:53

I will do it this week. Okay. I don't think we're in

Speaker 1: 00:05:57

any rush for it right?

Speaker 0: 00:05:58

Yeah no, if we want to actually change things, maybe before it starts getting used. But in my last comment last week, I created a gist with explanation and details and examples. So I'll let you go through it. And Basically, I think that what the spec is currently saying is correct. It works as, at least, it looks like it works as I expected. It lets the sender add a random number of nodes if they want to. So, I think it's, I think what the spec currently does is okay. But we need to make sure of that.

Speaker 1: 00:06:31

To be clear I have no desire to change anything. I'm happy to describe what people already do. There's like it might or might not be the cleanest possible outcome but it doesn't really matter. I just want to have to describe what people do.

Speaker 0: 00:06:44

Okay then Anyone should just have a look at the gist I put in the comment and see if it seems to make sense or if I missed something. And I think other pairs of eyes would be really helpful. Now I'll put the link here. But it takes some time. You should grab some coffee and spend at least half an hour on that to make sure that it makes sense. So yeah. So yeah, we'll keep tracking that directly on the PR. Nothing more to do on that one. So the next step, Antoine added a PR to harmonize the max HTLC expiry across implementations, if I understood correctly?

Speaker 1: 00:07:26

Yeah, that basically we're out of, like you wanted to increase the amount that we were defaulting to, to like four days or something. And my response was basically like, there's no way other nodes are actually going to keep routing through us if we do that. So I don't know that we need to actually write it. Sorry?

Speaker 0: 00:07:44

You mean selecting you in pathfinding or allowing to relay HTLCs at intermediate nodes?

Speaker 1: 00:07:49

The one that we announce, i.e. The one that we put in their channel update and tell people you have to add at least this delta. I don't know that there's a reason to put it in the bolts necessarily, but we do need... There should be some way for everyone to be on the same page in terms of what their route finding limits are basically like how high can we set it it really is a question of like how high can we set it before people start not running through us.

Speaker 0: 00:08:20

Yeah I agree. I think this is an issue because we mainly started way too low. I think initially years ago people tended to put low CLTVX by read-down-time because HLC tended to get stuck because of bugs and it was really annoying if they were stuck for too long. It's not happening that much. It can still happen in the malicious case. So that's why we don't want to infinite numbers there either. But what's the right balance? Yeah, I don't know. And so here, but here the value that Antoine is, wants to set is what we, is when we are an intermediate node, the difference between the nlock time of HTLC and the current block height, right?

Speaker 1: 00:09:06

Oh, I don't see why that needs to be. I mean, I guess that could be specified too. But I mean, that's probably just at least the delta. So it doesn't really matter, right?

Speaker 0: 00:09:19

Yeah, because I think it makes sense, because for example, we're changing Internet Claire to this value to be 2016 blocks like others because we're starting to see that everyone is raising their CL to be expired Delta, but we were before that at 1008, which means that if someone selected a route that exceeded 1, 008, we just would not accept to relay that, and people have no way of knowing that, which is bad. So we need a high enough number here, but this is also-

Speaker 1: 00:09:44

It is different, right? So like delta from the current block height to future block height to the CLTB for an intermediate node is different from the CLTB delta, right?

Speaker 0: 00:10:01

Yeah, but they actually matter because if you select my node in a route that has further down the path an LDK node that uses a CLC with 400 blocks, then the delta between the current block and the end of time in my HCLC is going to be big, and I'm still going to reject that HCLC instead of relaying.

Speaker 1: 00:10:22

Oh, right. Yes, I wonder if we should just specify then that, like, nodes have to have a delta from the current height that's equal to or greater than the delta announced in the channel update?

Speaker 0: 00:10:42

It will be. Yeah, it will be. But the thing is, it has to take into account other channels further down the road that you don't know about? Oh you mean in our channel?

Speaker 1: 00:10:52

Oh you mean the too far away thing, not the... Oh yeah, yeah. Sorry, I had the direction confused. Yeah, I don't see a reason why not to just, yeah, I mean, I guess we should set that, but I'm less concerned about that, more concerned about just the like total CLTV delta to begin with.

Speaker 0: 00:11:13

Okay. I don't think we actually have a rest friction here. We just slightly penalize, at least in a care, we just slightly penalize longer deltas, but that's customizable. And I think our default is to actually not penalize that much. And now-

Speaker 1: 00:11:33

I think someone dug in the LLD and concluded that they do the same. I kind of be concerned about that though. Like it's too low. I assume you also have a separate check for like it can't go over some threshold.

Speaker 0: 00:11:48

Mm hmm.

Speaker 1: 00:11:52

Yeah. I think we're gonna move or well, I intend to and probably won't ever get around to writing the code to just saying that there's a limit per hop. Per hop. It's not really like users might care about the total but it's kind of awkward in Dijkstra's to have like a total limit that you randomly hit at some point even if you penalize in your you know scoring so you might just have a hard limit per hop anyway.

Speaker 0: 00:12:20

Yeah okay okay I see. So yeah probably let justover chime in on that PR and we'll follow up later on that one. All right So then if Rusty is not here, I don't know if we have anything to add on Onion messages or authors. Anyone has something to add or should we just move on to the peons.

Speaker 2: 00:12:55

About the test vectors, they essentially are back

Speaker 0: 00:12:59

32

Speaker 2: 00:13:01

encoding and different line continuations. But there really isn't anything about like, are these TLVs like semantically correct? I'm not sure if that is intended

Speaker 3: 00:13:12

or we want to expand on those.

Speaker 0: 00:13:15

No, I think it's just that Rusty needs to update them and expand them but just didn't have the time to do so. So I think yeah your point from last meeting still stands that the test vectors need some love.

Speaker 2: 00:13:25

Well yeah, my point was more about they had old like field like recurrences But

Speaker 3: 00:13:31

I didn't really push on the fact that they weren't really expensive. But yeah, maybe I should. I'll add a comment to the PR.

Speaker 0: 00:13:39

OK. And I think we'll just follow up there. I think the next step is first to merge the annual messages so that the files is going to be based directly on master. It's going to be a bit easier to handle all the comments and rebases. All right, so next up, just exposure threshold. It still needs some love if someone can spend some time to review the latest state of the PR and either hack it or make comments on it to see what we need to change to actually merge that because yeah, that's something we already implemented. So it really makes sense to have it in the spec in my opinion. But yeah, so people, add it to your to-do list. Then do we have something new about Taproot and Taproot Gossip?

Speaker 1: 00:14:31

Sounds like we're just gonna boot that to New York since Ralo never... Wait, is Lalo here? No. Since he never got around to starting a mailing list thread and the mailing list is now filled with spam. Okay.

Speaker 0: 00:14:47

Does anyone know when we...

Speaker 1: 00:14:49

I know there was some... Sorry, before we move on. I know there was some conversation about the taproot spec itself, not the gossip. I know Wilmar and Arik and Lalo were going back and forth on that. I don't know what the state of that is and Lalo's not even here, so I'm not sure it's worth taking too much into.

Speaker 4: 00:15:12

Yeah, it's not worth it. You can just table it for next time.

Speaker 0: 00:15:16

Okay, And does anyone have some news about when we can expect to have a Music 2 in LipsecP and not the LipsecP-ZKP? No news on that.

Speaker 4: 00:15:30

In ZKP or in the upstream, like official Bitcoin Core one?

Speaker 0: 00:15:34

Yeah, I thought in the long run they agreed that they would once it would be stabilized that they would add it to LeapSec P1 but I don't know the state of that. We were kind of waiting on that one to get started on taproot.

Speaker 4: 00:15:52

I think I was chatting with Sanket recently. I may be mistaken, but I think there is some opposition to adding music at least in the near term because of wanting to do constant time operations on some stuff that's not currently supported.

Speaker 1: 00:16:15

We can get it in ZKP sooner, right? So it's not really a huge deal.

Speaker 4: 00:16:19

Well, it's in ZKP already.

Speaker 1: 00:16:22

Okay, okay. Some people should just use ZKP.

Speaker 0: 00:16:25

Yeah, okay. So we consider using ZKP then. Okay. And about taproot-gossip, is there something to add or open questions that could be resolved?

Speaker 1: 00:16:46

Looks like neither Al nor Lalo are here, so probably not.

Speaker 0: 00:16:51

Yep, all right. Can I play a sentence placing? I think also only for Blockstreaming us. So I'm not sure we are going to be able to do much tonight. Internal reestablish, but else, BTS, dual funding. I don't know if there's some feedback on dual funding, if there's been progress on the LDK side or LND side.

Speaker 1: 00:17:20

I don't know if Duncan's here, but he started doing some amount of work implementing the dual funding flow. It's still a little early and we have to do some, some internal refactors first, but you know, it's probably There will likely be something to look at in New York Probably not merged, but at least something that might be Maybe I shouldn't speak for Duncan, but something that might be worth trying to do testing on in the art. Okay, cool

Speaker 0: 00:17:55

Yeah, then I don't have anything else that I wanted to cover specifically. So if anyone has anything that they want to cover, just now is the time.

Speaker 1: 00:18:11

The mailing list went back on moderation. I don't know if it's worth talking about that in any form. Obviously there's some garbage spam but just figured I'd mention it.

Speaker 0: 00:18:25

Yeah and there hasn't been a lot of volume on the mailing list and there's nothing that has been added since then So I think we can handle the moderation manually. We're enough moderators right now. So that sounds good. Yeah, anything else interesting that has happened in the last two weeks in either your implementation or something else?

Speaker 1: 00:18:56

Not really. I mean, I did want to, I don't know if it's worth talking about, but we're going to have to make a decision soon on whether or not to do that really fail your back of HTLCs like if you forwarded an HTLC and you had to force close the outbound edge channel and the force closer commitment transaction or the HGLC timeout have not yet confirmed. Do you fail back the inbound edge? We haven't had an LDK discussion about it yet, but we probably will next Monday at our meeting. But if anyone else has opinions for whether or not They're going to do a similar thing. I would love to hear thoughts.

Speaker 0: 00:19:50

Yeah, we've been doing it for a very nuclear and I know that first, he agreed to that. You agree.

Speaker 1: 00:19:54

You say, didn't you say you only did it after the commitment transaction confirmed?

Speaker 0: 00:19:59

No. Yeah. Right. Yeah.

Speaker 1: 00:20:01

Right. So I'd also be interested in whether you would change that too.

Speaker 0: 00:20:12

Yeah, I don't know. Because yeah, We're aggressively trying to get the transaction, the commitment transaction to confirm.

Speaker 1: 00:20:20

Yeah, it may just be kind of a more of a pre-anchor issue. I'm not sure. I know that this was, I think the biggest issue. My estimates were that this was by far the biggest issue for force closures, or at least easily avoidable issue for force closures, when the fees were rapidly going up. Again, this might have been more of a pre-anchor channel issues, but I think it was the biggest issue, so I don't know.

Speaker 0: 00:20:51

Okay. Yeah. I need to think about it and to see if we add that for the case of a commitment transaction is that confirmed. I know that Rusty created an issue on the CRN and indicating that he wanted to do something like that, but he didn't specify whether he would do it also if a commitment transaction wasn't confirmed or need the case where the HTLC timeout wasn't confirmed.

Speaker 1: 00:21:12

Yeah. I'm not sure. I'm trying to think of what other force closures there were. Is that, is the LND bug which they've now fixed and I don't know if they've done a release yet. So make sure your peers are running LND 16.3 if you have LND peers.

Speaker 0: 00:21:39

It's probably going to help and we turned on ignoring internal errors from LND. So that's going to

Speaker 1: 00:21:47

help as well. Oh really? I haven't seen... I don't know that I've seen internal errors causing force closes on recent versions of LND.

Speaker 0: 00:21:59

We've seen people reporting that they received such errors and the NKL was actually first closing when it received an error. So, we had a few people.

Speaker 1: 00:22:07

Yeah, you're supposed to first close when you receive an error.

Speaker 0: 00:22:09

Yeah, but we had too many reports of people being affected by that. Mostly one guy who was playing a lot with rebalancing tools and making a lot of HDLCs, so I think he was hitting a lot of edge cases on his peers, so he was probably hitting most of the MND bugs when he was trying to push the limits. A lot of his channels eventually foreclosed, And he lost a lot of money on that.

Speaker 1: 00:22:33

So that sounds like a him issue rather than something to work around.

Speaker 0: 00:22:37

Yeah, I agree as well. But

Speaker 1: 00:22:41

yeah, I mean, they did have that bug a while back where they were treating warnings as errors. But I think they fixed that a while back. That might have been a year ago now.

Speaker 0: 00:22:49

Yeah I haven't seen that reported. So apart from that maybe we'll discuss it more in...

Speaker 1: 00:23:03

Oh I was going to mention one other force closure case.

Speaker 3: 00:23:07

I was trying to remember what

Speaker 1: 00:23:08

it was. So one question is, if you forwarded an HTLC and it's been tossed on both the inbound and outbound edge, does it make sense to have a feature bit to say, I'm not going to foreclose on that. And if your inbound edge has set that feature bit, then don't foreclose. It's tricky to get right because like periods can change. So maybe you only do it for anchors or for very, very small HTLCs that are going to remain dust. But it's kind of weird that we'd like to not force close if we have an outbound dust HTLC that's expired, because why would we force close that, but the inbound edge is going to force close the node that sent the HTLC to us so we can have to So would people be open to some kind of feature bit saying, you know, this HGLC is less than I don't know, whatever, 100 sats, like, just a feature bit saying I will not force close on an HGLC less than 100 sats and you can like I mean you might have to set that in the like update at htlc because it has to propagate all the way down the route I don't know

Speaker 0: 00:24:28

aren't we going through a lot of effort instead of fixing the root cause that there's just no reason that this HTLC is not failed back?

Speaker 1: 00:24:38

Well, I mean, yeah, it might just be that your peers offline, right?

Speaker 0: 00:24:42

Yeah, but should your peers stay offline for hours?

Speaker 1: 00:24:46

Well, if you're restarting an LND node.

Speaker 0: 00:24:49

Yeah, but that's crazy. That's what should be fixed. Restarting an LND node will take ages. It will take minutes to restart.

Speaker 1: 00:24:58

I agree. I agree. But anyway, it was just a thought because I thought like, we'll just stop force closing and then realized, of course, you can't do that because the inbound edge will also force close. So you're right, it's probably way too much effort, but it's gross.

Speaker 0: 00:25:14

But yeah, Maybe not if it's a good short-term solution so that...

Speaker 1: 00:25:19

Yeah, I mean, I think it's one of those things where it's like, if we were redesigning everything today, we would set some floor and say like, no one will ever force close under 100 sats or something. But is it worth the effort to add that now? Probably not so much.

Speaker 0: 00:25:39

Yeah, because yeah. It's mostly that if you're, how much of those first causes would actually be avoided by that because if a peer, if one of the peer is not failing these dust HTLCs are there also non-dust HTLCs that are not failed and should be failed which will make you force close anyway?

Speaker 1: 00:25:57

Right it's more a question of just like what is the proportion of HTLCs that are under this threshold, where you would just outright not fail because you wouldn't force close because all your HTLCs are under the threshold.

Speaker 0: 00:26:11

Yeah. Yeah, let's discuss that one in New York and see if we really want to do something about it. By the way, I'm really ashamed because I'm looking at the picture I took from the last summit about our probing protection protocol, the Oakland protocol, and I'm reminded that I said I would implement it and it's still, it's been a year and it still has not been implemented on my side. I don't know if

Speaker 1: 00:26:44

anyone else will look at it. If you implement it, you'll get a nice little bump in whether LDK prefers to route through you.

Speaker 0: 00:26:53

But I actually implemented the part that we set the max HDLC in flight to 45% of the capacity, I think, by default. But they did not implement the other thing about actually reducing, artificially reducing the capacity of other channels, of parallel channels to to avoid people tracking HTLCs. And that's the interesting part, I think.

Speaker 1: 00:27:23

What was the other part? No, I thought it was just that.

Speaker 0: 00:27:26

No, what we had on the whiteboard is to first reduce your max HTLC in flight, then have a 50 milliseconds commit sign timer to actually batch some stuff, and then put a full ceiling on the actually max HCLC in flight. Whenever you send an HCLC through one channel, you choose two of our random outgoing channels, and you reduce their ceiling for, we said, 200 to 1, 000 milliseconds to make sure that people probing and trying to send through various channels cannot actually track where the HTLCs are going. By looking at what HTLCs are succeeding. Yeah, I don't remember the exact details.

Speaker 1: 00:28:07

You also create a fake secondary HTLC on another channel. Was that the?

Speaker 4: 00:28:12

Yeah.

Speaker 1: 00:28:15

Oh, yeah, we never did that either.

Speaker 0: 00:28:17

Yeah. And I've kept that directly under my nose in one of the screenshots I put under my nose so that I look at it almost every day, but then I still haven't implemented it.

Speaker 1: 00:28:30

Well, first FixForce closes and then we'll

Speaker 0: 00:28:34

Yeah. Yeah, it wasn't really a priority. You have too many other things. If there's nothing else, maybe we can end early tonight. Perfect. So I'm not sure if we Are we going to do the meeting in two weeks or should we just wait for New York?

Speaker 1: 00:29:07

Let's do the meeting in two weeks just in case there's something that we want to talk about before New York. All right. Like if we want to have any discussions about topic ideas or pre-discussions or whatever.

Speaker 0: 00:29:22

All right, that'll create the tracking issue. All right, then thanks guys and see you next time.
